{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек"
      ],
      "metadata": {
        "id": "54o6iOvAA_8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaN8i6q9R6NJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "from rdkit.Chem import Descriptors, MolFromSmiles, rdFingerprintGenerator as fp\n",
        "\n",
        "from chython import smiles\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pyl\n",
        "from rdkit import Chem\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from rdkit.Chem import Descriptors, Lipinski, rdMolDescriptors\n",
        "from rdkit.Chem.EState import EState_VSA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Lipinski, rdMolDescriptors, AllChem, MACCSkeys\n",
        "from typing import List, Dict, Optional\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO добавить prepare\n",
        "class Data:\n",
        "    def __init__(self, path_train, path_test):\n",
        "        self._train = Path(path_train)\n",
        "        self._test = Path(path_test)\n",
        "\n",
        "    def load(self):\n",
        "        train_data = pd.read_csv(self._train)\n",
        "        test_data = pd.read_csv(self._test)\n",
        "\n",
        "        return train_data, test_data\n",
        "\n",
        "\n",
        "data_loader = Data('final_train_data_04_14_201_new_alex.csv', 'final_test_data_04_14_201_new_alex.csv')\n",
        "train_data, test_data = data_loader.load()\n",
        "y = train_data['LogP']\n",
        "\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "68j4qR0N_Tv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('final_train_data_04_14_201_new_alex.csv')\n",
        "test_data = pd.read_csv('final_test_data_04_14_201_new_alex.csv')"
      ],
      "metadata": {
        "id": "i8EQbFvHSy_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "4TB4uGBAsp2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "joDIj2YoTn79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(train_data.columns)"
      ],
      "metadata": {
        "id": "MVdCQiIpCJt3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_descriptors = Descriptors.descList[:100]\n",
        "# descriptor_names = [name for name, func in all_descriptors]\n",
        "\n",
        "# def compute_descriptors(smiles):\n",
        "#     mol = Chem.MolFromSmiles(smiles)\n",
        "#     if mol is None:\n",
        "#         return [None] * len(all_descriptors)\n",
        "#     return [func(mol) for _, func in all_descriptors]\n",
        "\n",
        "# def add_descriptors(df):\n",
        "#     desc_values = []\n",
        "#     valid_indices = []\n",
        "#     for i, smi in tqdm(enumerate(df['SMILES']), total=len(df)):\n",
        "#         desc = compute_descriptors(smi)\n",
        "#         if None not in desc:\n",
        "#             desc_values.append(desc)\n",
        "#             valid_indices.append(i)\n",
        "#     desc_df = pd.DataFrame(desc_values, columns=descriptor_names)\n",
        "#     df_clean = df.iloc[valid_indices].reset_index(drop=True)\n",
        "#     df_clean = pd.concat([df_clean.reset_index(drop=True), desc_df], axis=1)\n",
        "#     return df_clean\n",
        "\n",
        "# train_data = add_descriptors(train_data)\n",
        "# test_data = add_descriptors(test_data)"
      ],
      "metadata": {
        "id": "rYYTHAvTTn3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "aboVKrNoTn06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "B7Dkwuj2Tnym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_data.drop(columns=['LogP'])\n",
        "y_train = train_data['LogP']\n",
        "X_test = test_data"
      ],
      "metadata": {
        "id": "441rXaYqTnn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверми пропуски\n",
        "print(X_train.isna().sum().sum(), X_test.isna().sum().sum())"
      ],
      "metadata": {
        "id": "xr-xr9yrxLez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Заполним пропуски средним\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_train.mean())"
      ],
      "metadata": {
        "id": "PskCOb1TxD3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверми пропуски еще раз\n",
        "print(X_train.isna().sum().sum(), X_test.isna().sum().sum())"
      ],
      "metadata": {
        "id": "CC8d2nMpxVAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "catboost_regressor = CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE', verbose=0, random_seed=42)\n",
        "\n",
        "param_catboost = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_search_catboost = GridSearchCV(catboost_regressor, param_catboost, cv=5,  scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "grid_search_catboost.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xkX1ApDBxc6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_catboost.best_params_"
      ],
      "metadata": {
        "id": "_3roZQpQxifR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "LNNBqLYGAcvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказание на тестовых данных\n",
        "test_data = pd.read_csv('final_test_data_04_14_201_new_alex.csv')\n",
        "predictions = best_model.predict(test_data)\n",
        "\n",
        "# Сохранение в формате submission (как в примере с SVM)\n",
        "submission_template = pd.read_csv('final_sample_submission80.csv')  # Используем тот же шаблон\n",
        "submission = submission_template.copy()\n",
        "submission['LogP'] = predictions\n",
        "submission.to_csv('predict333.csv', index=False)\n",
        "\n",
        "print(\"\\nСохранено.\")"
      ],
      "metadata": {
        "id": "FTemRVKZDU6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Результат модели на платформе: 0.691942**"
      ],
      "metadata": {
        "id": "pjJZe1cbK4Rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "TwBd9KQKsSRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.pipeline import Pipeline\n",
        "import optuna\n",
        "from optuna.exceptions import TrialPruned\n",
        "import multiprocessing as mp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import time\n",
        "\n",
        "\n",
        "def load_and_split_data(df: pd.DataFrame, target_col: str, test_size: float, random_state: int):\n",
        "    \"\"\"Загружает, разделяет признаки и цель, выполняет train/test split.\"\"\"\n",
        "    try:\n",
        "        X = df.drop(columns=[target_col])\n",
        "        y = df[target_col]\n",
        "    except KeyError:\n",
        "        print(f\"Ошибка: Целевая колонка '{target_col}' не найдена в DataFrame.\")\n",
        "        raise\n",
        "\n",
        "    stratify_option = None\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                        y,\n",
        "                                                        test_size=test_size,\n",
        "                                                        random_state=random_state,\n",
        "                                                        stratify=stratify_option)\n",
        "\n",
        "    print(f\"Данные разделены (полные): X_train_full: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, X.columns\n",
        "\n",
        "def create_pipeline(trial: optuna.trial.Trial) -> Pipeline:\n",
        "    \"\"\"Создает пайплайн sklearn с гиперпараметрами, предложенными Optuna.\"\"\"\n",
        "\n",
        "    variance_threshold = trial.suggest_float(\"variance_threshold\", 0.0, 0.1, log=False)\n",
        "    #kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\"])\n",
        "    kernel = trial.suggest_categorical(\"kernel\", [\"rbf\", \"poly\"])\n",
        "\n",
        "    C = trial.suggest_float(\"C\", 1e-4, 1e2, log=True)\n",
        "\n",
        "    if kernel == \"rbf\": # не учитываем kernel = 'linear' - с ним результат не высок\n",
        "        gamma = trial.suggest_categorical(\"gamma_rbf\", [\"scale\", \"auto\"])\n",
        "        # gamma_val = trial.suggest_float(\"gamma_rbf_val\", 1e-4, 1e1, log=True) # Альтернатива\n",
        "        svm_params = {\"kernel\": kernel, \"C\": C, \"gamma\": gamma}\n",
        "    elif kernel == \"poly\":\n",
        "        degree = trial.suggest_int(\"degree\", 2, 6)\n",
        "        gamma = trial.suggest_categorical(\"gamma_poly\", [\"scale\", \"auto\"])\n",
        "        svm_params = {\"kernel\": kernel, \"C\": C, \"degree\": degree, \"gamma\": gamma}\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('variance_filter', VarianceThreshold(threshold=variance_threshold)),\n",
        "        ('svm', SVR(**svm_params, cache_size=5000))\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "def objective(trial: optuna.trial.Trial, X_train: pd.DataFrame, y_train: pd.Series, cv_folds: int, scoring: str) -> float:\n",
        "    \"\"\"Целевая функция для Optuna.\"\"\"\n",
        "    pipeline = create_pipeline(trial)\n",
        "    try:\n",
        "        scores = cross_val_score(\n",
        "            pipeline, X_train, y_train,\n",
        "            cv=KFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE),\n",
        "            scoring=scoring, # здесь  'r_squared'\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        mean_score = np.mean(scores)\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Ошибка во время cross_val_score для триала {trial.number}: {e}\")\n",
        "\n",
        "        return -np.inf\n",
        "\n",
        "    # Обработка NaN/inf в скорах\n",
        "    if np.isnan(mean_score) or np.isinf(mean_score):\n",
        "        print(f\"Предупреждение: CV вернул NaN/inf для триала {trial.number}. Возвращаем плохое значение.\")\n",
        "\n",
        "        return -np.inf\n",
        "\n",
        "    return mean_score\n",
        "\n",
        "def train_final_model(best_params: dict, X_train_full: pd.DataFrame, y_train_full: pd.Series) -> Pipeline:\n",
        "    \"\"\"Обучает финальный пайплайн на ВСЕХ обучающих данных с лучшими параметрами.\"\"\"\n",
        "    print(\"\\n Обучение финальной модели с лучшими параметрами на ПОЛНОМ обучающем датасете...\")\n",
        "    variance_threshold = best_params['variance_threshold']\n",
        "    kernel = best_params['kernel']\n",
        "    C = best_params['C']\n",
        "\n",
        "    svm_params = {\"kernel\": kernel, \"C\": C}\n",
        "    if kernel == \"rbf\":\n",
        "        svm_params[\"gamma\"] = best_params['gamma_rbf']\n",
        "    elif kernel == \"poly\":\n",
        "        svm_params[\"degree\"] = best_params['degree']\n",
        "        svm_params[\"gamma\"] = best_params['gamma_poly']\n",
        "\n",
        "    final_pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('variance_filter', VarianceThreshold(threshold=variance_threshold)),\n",
        "        ('svm', SVR(**svm_params, cache_size=5000))\n",
        "    ])\n",
        "\n",
        "    final_pipeline.fit(X_train_full, y_train_full)\n",
        "    print(\"Финальная модель обучена.\")\n",
        "\n",
        "    try:\n",
        "        variance_filter_step = final_pipeline.named_steps['variance_filter']\n",
        "        feature_mask = variance_filter_step.get_support()\n",
        "        n_features_selected = sum(feature_mask)\n",
        "        print(f\"Количество признаков после VarianceThreshold (threshold={variance_threshold:.4f}): {n_features_selected} / {X_train_full.shape[1]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Не удалось получить количество признаков после фильтрации: {e}\")\n",
        "\n",
        "    return final_pipeline\n",
        "\n",
        "def evaluate_model(pipeline: Pipeline, X_test: pd.DataFrame, y_test: pd.Series):\n",
        "    \"\"\"Оценивает обученный пайплайн на тестовых данных и выводит метрики.\"\"\"\n",
        "    print(\"\\nОценка качества лучшей модели на тестовой выборке...\")\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred) # R2 считаем для отчета\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"Coefficient of Determination (R²): {r2:.4f}\")\n",
        "    return y_pred, {'mse': mse, 'rmse': rmse, 'r2': r2}\n",
        "\n",
        "def plot_results(y_test: pd.Series, y_pred: np.ndarray):\n",
        "    \"\"\"Строит график предсказанных vs реальных значений.\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=y_test, y=y_pred, alpha=0.7)\n",
        "    plt.xlabel(\"Actual LogP\", fontsize=12)\n",
        "    plt.ylabel(\"Predicted LogP\", fontsize=12)\n",
        "    plt.title(\"Optimized SVM Regression (Max R²): Predicted vs Actual LogP\", fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    min_val = min(y_test.min(), y_pred.min())\n",
        "    max_val = max(y_test.max(), y_pred.max())\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', linewidth=2)\n",
        "    plt.show()\n",
        "\n",
        "def plot_optuna_results(study: optuna.study.Study):\n",
        "    \"\"\"Визуализирует результаты оптимизации Optuna.\"\"\"\n",
        "    try:\n",
        "        if optuna.visualization.is_available():\n",
        "            fig1 = optuna.visualization.plot_optimization_history(study)\n",
        "            fig1.show()\n",
        "\n",
        "            fig2 = optuna.visualization.plot_param_importances(study)\n",
        "            fig2.show()\n",
        "        else:\n",
        "            print(\"Optuna visualization is not available. Please install plotly.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при построении графиков Optuna: {e}\")\n",
        "\n",
        "TARGET_COLUMN = 'LogP'\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "CV_FOLDS = 5\n",
        "\n",
        "\n",
        "SCORING = 'r2'\n",
        "\n",
        "N_TRIALS = 100\n",
        "OPTUNA_TIMEOUT = 10000\n",
        "\n",
        "USE_SUBSET_FOR_OPTUNA = True\n",
        "OPTUNA_SUBSET_FRACTION = 0.5\n",
        "STRATIFY_SUBSET = False\n",
        "\n",
        "desc_df = pd.read_csv('final_train_data_04_14_201_new_alex.csv')\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test, feature_names = load_and_split_data(desc_df,\n",
        "                                                                                TARGET_COLUMN,\n",
        "                                                                                TEST_SIZE,\n",
        "                                                                                RANDOM_STATE)\n",
        "\n",
        "if USE_SUBSET_FOR_OPTUNA and OPTUNA_SUBSET_FRACTION < 1.0:\n",
        "    print(f\"\\n Используется подвыборка ({OPTUNA_SUBSET_FRACTION * 100:.0f}%) для оптимизации Optuna.\")\n",
        "    stratify_option = None\n",
        "    X_train_subset, _, y_train_subset, _ = train_test_split(X_train_full,\n",
        "                                                            y_train_full,\n",
        "                                                            train_size=OPTUNA_SUBSET_FRACTION,\n",
        "                                                            random_state=RANDOM_STATE + 1,\n",
        "                                                            stratify=stratify_option)\n",
        "\n",
        "    print(f\"Размер подвыборки для Optuna: X_train_subset: {X_train_subset.shape}\")\n",
        "    X_train_optuna = X_train_subset\n",
        "    y_train_optuna = y_train_subset\n",
        "else:\n",
        "    print(\"\\n Используется ПОЛНЫЙ обучающий набор данных для оптимизации Optuna.\")\n",
        "    X_train_optuna = X_train_full\n",
        "    y_train_optuna = y_train_full\n",
        "\n",
        "direction = \"maximize\"\n",
        "study = optuna.create_study(direction=direction)\n",
        "\n",
        "print(f\"\\nЗапуск оптимизации Optuna ({N_TRIALS} триалов, цель: максимизировать {SCORING})...\")\n",
        "start_time = time.time()\n",
        "\n",
        "study.optimize(lambda trial: objective(trial, X_train_optuna, y_train_optuna, CV_FOLDS, SCORING),\n",
        "               n_trials=N_TRIALS,\n",
        "               timeout=OPTUNA_TIMEOUT,\n",
        "               n_jobs=1)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Оптимизация завершена за {end_time - start_time:.2f} секунд.\")\n",
        "\n",
        "\n",
        "print(\"\\n Результаты оптимизации Optuna:\")\n",
        "\n",
        "print(f\"Лучшее значение метрики ({SCORING}): {study.best_value:.4f}\")\n",
        "\n",
        "\n",
        "print(\"Лучшие гиперпараметры:\")\n",
        "best_params = study.best_params\n",
        "for key, value in best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "final_model_pipeline = train_final_model(best_params, X_train_full, y_train_full)\n",
        "\n",
        "\n",
        "joblib.dump(final_model_pipeline, 'final_svm_optuna_pipeline_04_14_201_new_alex.joblib')\n",
        "# joblib.dump(study, 'optuna_study.joblib')\n",
        "\n",
        "y_pred_test, test_metrics = evaluate_model(final_model_pipeline, X_test, y_test)\n",
        "\n",
        "plot_results(y_test, y_pred_test)\n",
        "plot_optuna_results(study)\n",
        "\n",
        "test_data = pd.read_csv('final_test_data_04_14_201_new_alex.csv')\n",
        "predictions = final_model_pipeline.predict(test_data)\n",
        "submission = pd.read_csv('final_sample_submission80.csv')\n",
        "submission.LogP = predictions\n",
        "submission.to_csv('prediction_04_14_201_new_alex.csv', index=False)\n",
        "\n",
        "\n",
        "print(\"\\nСкрипт успешно выполнен!\")"
      ],
      "metadata": {
        "id": "1_93tXSHsR7E",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тестирую модели на новых данных, обавление 200 фичей + новые"
      ],
      "metadata": {
        "id": "-T93jAD6kR1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train_data_clean_rdkit.csv')\n",
        "test = pd.read_csv('test_rdkit_filtered.csv')"
      ],
      "metadata": {
        "id": "SPR1mKqOkQ_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "Nm5bqXY9kQ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "L5htoyq8kQ1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
        "\n",
        "# Проверка на наличие SMILES\n",
        "assert 'SMILES' in train.columns, \"Нет столбца SMILES в train\"\n",
        "assert 'SMILES' in test.columns, \"Нет столбца SMILES в test\"\n",
        "\n",
        "# Дескрипторы, которые мы добавим\n",
        "extra_descriptors = {\n",
        "    'MolMR': Descriptors.MolMR,\n",
        "    'NumValenceElectrons': Descriptors.NumValenceElectrons,\n",
        "    'MaxPartialCharge': Descriptors.MaxPartialCharge,\n",
        "    'MinPartialCharge': Descriptors.MinPartialCharge,\n",
        "    'NumRadicalElectrons': Descriptors.NumRadicalElectrons,\n",
        "    'NumAromaticRings': rdMolDescriptors.CalcNumAromaticRings,\n",
        "    'NumAliphaticRings': rdMolDescriptors.CalcNumAliphaticRings,\n",
        "    'NumSaturatedRings': rdMolDescriptors.CalcNumSaturatedRings\n",
        "}\n",
        "\n",
        "# Функция вычисления признаков\n",
        "def compute_features(smiles_list):\n",
        "    descriptors = []\n",
        "    morgan_fp = []\n",
        "    maccs_fp = []\n",
        "\n",
        "    for smi in tqdm(smiles_list, desc=\"Обработка молекул\"):\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            descriptors.append([np.nan]*len(extra_descriptors))\n",
        "            morgan_fp.append([0]*256)\n",
        "            maccs_fp.append([0]*167)\n",
        "            continue\n",
        "\n",
        "        # Основные дескрипторы\n",
        "        desc_values = [func(mol) for func in extra_descriptors.values()]\n",
        "        descriptors.append(desc_values)\n",
        "\n",
        "        # Morgan Fingerprint\n",
        "        fp = GetMorganFingerprintAsBitVect(mol, radius=2, nBits=256)\n",
        "        morgan_fp.append(list(fp))\n",
        "\n",
        "        # MACCS Keys\n",
        "        maccs = MACCSkeys.GenMACCSKeys(mol)\n",
        "        maccs_fp.append(list(maccs)[1:])  # первый элемент всегда 0\n",
        "\n",
        "    desc_df = pd.DataFrame(descriptors, columns=extra_descriptors.keys())\n",
        "    morgan_df = pd.DataFrame(morgan_fp, columns=[f'Morgan_{i}' for i in range(256)])\n",
        "    maccs_df = pd.DataFrame(maccs_fp, columns=[f'MACCS_{i}' for i in range(1, 167)])\n",
        "\n",
        "\n",
        "    return pd.concat([desc_df, morgan_df, maccs_df], axis=1)\n",
        "\n",
        "# Применение к train и test\n",
        "print(\"Вычисляем фичи для train...\")\n",
        "train_features = compute_features(train['SMILES'])\n",
        "train_extended = pd.concat([train, train_features], axis=1)\n",
        "train_extended.to_csv(\"train_extended.csv\", index=False)\n",
        "\n",
        "print(\"Вычисляем фичи для test...\")\n",
        "test_features = compute_features(test['SMILES'])\n",
        "test_extended = pd.concat([test, test_features], axis=1)\n",
        "test_extended.to_csv(\"test_extended.csv\", index=False)\n",
        "\n",
        "print(\"Файлы train_extended.csv и test_extended.csv сохранены!\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GjYxmT_pkQyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_new"
      ],
      "metadata": {
        "id": "0iibfDJzkQtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_new"
      ],
      "metadata": {
        "id": "cTnA5S-bkQqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_data_new.drop(columns=['SMILES', 'LogP'])\n",
        "y_train = train_data_new['LogP']\n",
        "X_test = test_data_new.drop(columns=['ID', 'SMILES'])"
      ],
      "metadata": {
        "id": "KjcKqxBBkQfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверми пропуски\n",
        "print(X_train.isna().sum().sum(), X_test.isna().sum().sum())"
      ],
      "metadata": {
        "id": "G3vPRU60kQc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Заполним пропуски средним\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_train.mean())"
      ],
      "metadata": {
        "id": "PT8WEyXVkQaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.isna().sum().sum(), X_test.isna().sum().sum())"
      ],
      "metadata": {
        "id": "yhXoU61-kQXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "catboost_regressor = CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE', verbose=0, random_seed=42)\n",
        "\n",
        "param_catboost = {\n",
        "    'n_estimators': [200, 300, 350],\n",
        "    'learning_rate': [0.01],\n",
        "    'depth': [3, 5, 7],\n",
        "    'l2_leaf_reg': [3, 5]\n",
        "}\n",
        "\n",
        "grid_search_catboost2 = GridSearchCV(catboost_regressor, param_catboost, cv=5,  scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "grid_search_catboost2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "BBeojnyCkQUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model22 = grid_search_catboost2.best_estimator_"
      ],
      "metadata": {
        "id": "Tt45QlH1p2JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred22 = best_model22.predict(X_test)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data_new['ID'],\n",
        "    'LogP': y_test_pred22\n",
        "})\n",
        "submission.to_csv('sub333.csv', index=False)"
      ],
      "metadata": {
        "id": "ii6E0x-DkQQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XzZbiEWsVSNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.fit(X_tr, y_tr)"
      ],
      "metadata": {
        "id": "5S2rxpapVSIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = best_model.predict(X_val)"
      ],
      "metadata": {
        "id": "JcWjs1HDVSGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "rmse = root_mean_squared_error(y_val, y_val_pred)\n",
        "print(\"RMSE на отложенной валидации:\", rmse)"
      ],
      "metadata": {
        "id": "9KcxH5TBVR_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Score модели в Leaderboard: **0.66085**"
      ],
      "metadata": {
        "id": "DWJYWugBWnHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Тест №3. Добавление новых дескрипторов**"
      ],
      "metadata": {
        "id": "PDGkbp1HcORq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "M3Eqz7tlXCfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Тест RDKit**"
      ],
      "metadata": {
        "id": "B5JJ83kK4Tjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Добавим больше дескрипторов**"
      ],
      "metadata": {
        "id": "hr-DZl2BaOUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data2 = pd.read_csv('/content/rdkit_standardized_data_without_duplicates.csv')\n",
        "test_data2 = pd.read_csv('/content/test_rdkit_standardized_data_without_duplicates.csv')"
      ],
      "metadata": {
        "id": "YQYp45EmAcgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data2"
      ],
      "metadata": {
        "id": "Z-fZJQl18tix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data2"
      ],
      "metadata": {
        "id": "V7qKfCsf8vva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для безопасного преобразования SMILES\n",
        "def clean_smiles(smiles):\n",
        "    if pd.isna(smiles):\n",
        "        return None\n",
        "    try:\n",
        "        return str(smiles).strip()\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Все возможные дескрипторы RDKit\n",
        "all_descriptors = Descriptors.descList\n",
        "descriptor_names = [name for name, func in all_descriptors]\n",
        "\n",
        "def compute_descriptors(smiles):\n",
        "    # Очистка и проверка SMILES\n",
        "    clean_smi = clean_smiles(smiles)\n",
        "    if clean_smi is None:\n",
        "        return None\n",
        "\n",
        "    mol = Chem.MolFromSmiles(clean_smi)\n",
        "    if mol is None:\n",
        "        return None\n",
        "\n",
        "    desc_values = {}\n",
        "    for name, func in all_descriptors:\n",
        "        try:\n",
        "            desc_values[name] = func(mol)\n",
        "        except:\n",
        "            desc_values[name] = None\n",
        "    return desc_values\n",
        "\n",
        "def add_descriptors(df):\n",
        "    results = []\n",
        "    valid_indices = []\n",
        "    invalid_count = 0\n",
        "\n",
        "    for i, smi in tqdm(enumerate(df['SMILES']), total=len(df)):\n",
        "        desc = compute_descriptors(smi)\n",
        "        if desc is not None:\n",
        "            results.append(desc)\n",
        "            valid_indices.append(i)\n",
        "        else:\n",
        "            invalid_count += 1\n",
        "\n",
        "    print(f\"\\nПропущено {invalid_count} невалидных SMILES\")\n",
        "\n",
        "    # Создаем DataFrame и очищаем его\n",
        "    desc_df = pd.DataFrame(results)\n",
        "\n",
        "    # Удаляем полностью пустые столбцы\n",
        "    desc_df = desc_df.dropna(axis=1, how='all')\n",
        "\n",
        "    # Удаляем столбцы с постоянными значениями\n",
        "    desc_df = desc_df.loc[:, desc_df.nunique() > 1]\n",
        "\n",
        "    # Объединяем с исходными данными\n",
        "    df_clean = df.iloc[valid_indices].reset_index(drop=True)\n",
        "    df_clean = pd.concat([df_clean, desc_df], axis=1)\n",
        "\n",
        "    actual_descriptors = desc_df.columns.tolist()\n",
        "    print(f\"Успешно добавлено {len(actual_descriptors)} дескрипторов\")\n",
        "\n",
        "    return df_clean, actual_descriptors\n",
        "\n",
        "# Обработка данных\n",
        "print(\"Обработка тренировочных данных...\")\n",
        "train_data2, train_descriptors = add_descriptors(train_data2)\n",
        "\n",
        "print(\"\\nОбработка тестовых данных...\")\n",
        "test_data2, _ = add_descriptors(test_data2)\n",
        "\n",
        "# Сохраняем обработанные данные\n",
        "train_data2.to_csv('train_data_processed.csv', index=False)\n",
        "test_data2.to_csv('test_data_processed.csv', index=False)"
      ],
      "metadata": {
        "id": "5fZ5avq33Xgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data2.head()"
      ],
      "metadata": {
        "id": "cNvfKjZ-ZQr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data2.head()"
      ],
      "metadata": {
        "id": "N_qNAZ-XZSbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_data2.drop(columns=['SMILES', 'LogP'])\n",
        "y_train = train_data2['LogP']\n",
        "X_test = test_data2.drop(columns=['ID', 'SMILES'])"
      ],
      "metadata": {
        "id": "dMKZpYRzYJcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверми пропуски\n",
        "print(X_train.isna().sum().sum(), X_test.isna().sum().sum())"
      ],
      "metadata": {
        "id": "OQutbrGfYJXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Заполним пропуски средним\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_train.mean())"
      ],
      "metadata": {
        "id": "7NyRzBsPYJUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверми пропуски еще раз\n",
        "print(X_train.isna().sum().sum(), X_test.isna().sum().sum())"
      ],
      "metadata": {
        "id": "cFP5jGmxYJPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "catboost_regressor = CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE', verbose=0, random_seed=42)\n",
        "\n",
        "param_catboost = {\n",
        "    'n_estimators': [200, 300, 350],\n",
        "    'learning_rate': [0.01],\n",
        "    'depth': [3, 5, 7],\n",
        "    'l2_leaf_reg': [3, 5]\n",
        "}\n",
        "\n",
        "grid_search_catboost2 = GridSearchCV(catboost_regressor, param_catboost, cv=5,  scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "grid_search_catboost2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "VeUI5zCKa_68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_catboost2.best_params_"
      ],
      "metadata": {
        "id": "8ZUN1k7la_49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2 = grid_search_catboost2.best_estimator_"
      ],
      "metadata": {
        "id": "BRRLM0JNa_2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "5RudsB8Ea_1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2.fit(X_tr, y_tr)"
      ],
      "metadata": {
        "id": "Vqx53B0oa_zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = best_model2.predict(X_val)"
      ],
      "metadata": {
        "id": "I_ccO9A7a_vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "rmse = root_mean_squared_error(y_val, y_val_pred)\n",
        "print(\"RMSE на отложенной валидации:\", rmse)"
      ],
      "metadata": {
        "id": "N2D7op5Ya_sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred2 = best_model2.predict(X_test)"
      ],
      "metadata": {
        "id": "-3IzO4sSa_qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'ID': test_data2['ID'],\n",
        "    'LogP': y_test_pred2\n",
        "})\n",
        "submission.to_csv('1.csv', index=False)"
      ],
      "metadata": {
        "id": "6dYiYAv7cieY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Score в Leaderboard: **0.65673**"
      ],
      "metadata": {
        "id": "9qpkhl6O2Tog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "zpRCvrdV2m5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Тест №4. Добавление новых фингерпринтов. Тест через стекинг**"
      ],
      "metadata": {
        "id": "fEBC21JA2qJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "w4RuJ6Tv2muu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data3 = pd.read_csv('train_data.csv')\n",
        "test_data3 = pd.read_csv('test_data.csv')"
      ],
      "metadata": {
        "id": "MQk2sluZcicN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение ключевых дескрипторов\n",
        "selected_descriptors = {\n",
        "    # Физико-химические\n",
        "    'MolWt': Descriptors.MolWt,\n",
        "    'MolLogP': Descriptors.MolLogP,\n",
        "    'MolMR': Descriptors.MolMR,\n",
        "    'TPSA': Descriptors.TPSA,\n",
        "    'LabuteASA': Descriptors.LabuteASA,\n",
        "\n",
        "    # Водородные связи\n",
        "    'NumHDonors': Lipinski.NumHDonors,\n",
        "    'NumHAcceptors': Lipinski.NumHAcceptors,\n",
        "\n",
        "    # Конституционные\n",
        "    'HeavyAtomCount': Descriptors.HeavyAtomCount,\n",
        "    'NumRotatableBonds': Descriptors.NumRotatableBonds,\n",
        "    'FractionCSP3': Descriptors.FractionCSP3,\n",
        "    'RingCount': rdMolDescriptors.CalcNumRings,\n",
        "\n",
        "    # EState индексы\n",
        "    'EState_VSA1': EState_VSA.EState_VSA1,\n",
        "    'EState_VSA2': EState_VSA.EState_VSA2,\n",
        "\n",
        "    # Дополнительные\n",
        "    'BalabanJ': Chem.GraphDescriptors.BalabanJ,\n",
        "    'BertzCT': Chem.GraphDescriptors.BertzCT\n",
        "}"
      ],
      "metadata": {
        "id": "1LZS7c772mLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Функция для вычисления дескрипторов и отпечатков\n",
        "def compute_features(smiles_list, n_bits=256):\n",
        "    mols = [Chem.MolFromSmiles(s) for s in smiles_list]\n",
        "    features = []\n",
        "\n",
        "    for mol in mols:\n",
        "        if mol is None:\n",
        "            features.append([None]*len(selected_descriptors) + [0]*n_bits)\n",
        "            continue\n",
        "\n",
        "        # Вычисляем дескрипторы\n",
        "        desc_values = []\n",
        "        for name, func in selected_descriptors.items():\n",
        "            try:\n",
        "                desc_values.append(func(mol))\n",
        "            except:\n",
        "                desc_values.append(None)\n",
        "\n",
        "        # Вычисляем отпечатки Моргана\n",
        "        fp = list(rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=n_bits))\n",
        "\n",
        "        features.append(desc_values + fp)\n",
        "\n",
        "    return pd.DataFrame(features,\n",
        "                       columns=list(selected_descriptors.keys()) + [f'FP_{i}' for i in range(n_bits)])"
      ],
      "metadata": {
        "id": "Ie2T1uk00724"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Вычисление фичей\n",
        "print(\"Вычисление фичей для тренировочных данных...\")\n",
        "train_features = compute_features(train_data3['SMILES'])\n",
        "train_features['LogP'] = train_data3['LogP'].values\n",
        "\n",
        "print(\"\\nВычисление фичей для тестовых данных...\")\n",
        "test_features = compute_features(test_data3['SMILES'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XIqg2bkV48DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Подготовка данных\n",
        "X_train = train_features.drop('LogP', axis=1)\n",
        "y_train = train_features['LogP']\n",
        "X_test = test_features\n",
        "\n",
        "# Разделение на группы фичей\n",
        "desc_cols = list(selected_descriptors.keys())\n",
        "fp_cols = [col for col in X_train.columns if col.startswith('FP_')]"
      ],
      "metadata": {
        "id": "zvGWtep74779"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Создание пайплайна\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('desc', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), desc_cols),\n",
        "    ('fp', 'passthrough', fp_cols)\n",
        "])\n",
        "\n",
        "# Базовые модели\n",
        "base_models = [\n",
        "    ('catboost', CatBoostRegressor(\n",
        "        iterations=1000,\n",
        "        depth=7,\n",
        "        l2_leaf_reg=5,\n",
        "        learning_rate=0.03,\n",
        "        random_seed=42,\n",
        "        verbose=0\n",
        "    )),\n",
        "    ('xgb', XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    ))\n",
        "]\n",
        "\n",
        "# Мета-модель\n",
        "meta_model = ElasticNet(alpha=0.001, l1_ratio=0.7)\n",
        "\n",
        "# Полный пайплайн\n",
        "final_model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selector', VarianceThreshold(threshold=0.15)),\n",
        "    ('stacking', StackingRegressor(\n",
        "        estimators=base_models,\n",
        "        final_estimator=meta_model,\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "ty0dKweG475j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "RIANt1sn6fGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Обучение модели\n",
        "final_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "sGbdzEFx47yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Предсказание и сохранение результатов\n",
        "test_pred = final_model.predict(X_test)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data3['ID'],\n",
        "    'LogP': test_pred\n",
        "})\n",
        "submission.to_csv('submission333.csv', index=False)\n",
        "\n",
        "print(\"\\nСохранено\")"
      ],
      "metadata": {
        "id": "3fKnKCuW41-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "HwHhMJ6m7OsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "v83-M-Ri7fmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавление новых фичей"
      ],
      "metadata": {
        "id": "DGg1XUb_3W5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm as tqdm_notebook # Используем для ноутбуков\n",
        "from tqdm import tqdm as tqdm_base # Для обычного скрипта\n",
        "import sys\n",
        "\n",
        "# --- 0. Настройка ---\n",
        "# Игнорирование предупреждений\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Выбор TQDM\n",
        "try: from tqdm.notebook import tqdm as tqdm_func\n",
        "except ImportError: from tqdm import tqdm as tqdm_func\n",
        "\n",
        "# Конфигурация путей (!!! ПРОВЕРЬ ИХ !!!)\n",
        "SOURCE_TRAIN_PATH = 'train_data_clean_rdkit (2).csv' # Вход: SMILES, LogP\n",
        "SOURCE_TEST_PATH = 'test_data.csv'                  # Вход: ID, SMILES\n",
        "SAMPLE_SUBMISSION_PATH = 'final_sample_submission80.csv' # Для получения ID теста, если их нет в test_data.csv\n",
        "\n",
        "OUTPUT_TRAIN_EXTENDED_PATH = 'train_features_extended.csv' # Выход: Расширенные признаки + LogP\n",
        "OUTPUT_TEST_EXTENDED_PATH = 'test_features_extended.csv'   # Выход: Расширенные признаки + ID\n",
        "OUTPUT_IMPUTER_PATH = 'feature_imputer.pkl'              # Сохраненный импутер\n",
        "\n",
        "# Параметры фингерпринтов\n",
        "ECFP_RADIUS = 2\n",
        "ECFP_NBITS = 2048\n",
        "FP_GENERATOR = GetMorganGenerator(radius=ECFP_RADIUS, fpSize=ECFP_NBITS) # Создаем генератор один раз\n",
        "\n",
        "# Имена колонок\n",
        "SMILES_COL = 'SMILES'\n",
        "TARGET_COL = 'LogP'\n",
        "ID_COL = 'ID'\n",
        "\n",
        "# --- 1. Функции ---\n",
        "\n",
        "def standardize_mol(mol):\n",
        "    \"\"\"Стандартизирует молекулу RDKit.\"\"\"\n",
        "    if mol is None: return None\n",
        "    try:\n",
        "        # Remove fragments\n",
        "        clean_mol = rdMolStandardize.Cleanup(mol)\n",
        "        # Get parent fragment\n",
        "        parent_clean_mol = rdMolStandardize.FragmentParent(clean_mol)\n",
        "        # Neutralize charges\n",
        "        uncharger = rdMolStandardize.Uncharger() # Handles multiple fragments if necessary\n",
        "        uncharged_parent_clean_mol = uncharger.uncharge(parent_clean_mol)\n",
        "        # Get canonical tautomer\n",
        "        te = rdMolStandardize.TautomerEnumerator() # Default parameters are usually good\n",
        "        canon_tautomer_mol = te.Canonicalize(uncharged_parent_clean_mol)\n",
        "        return canon_tautomer_mol\n",
        "    except Exception as e:\n",
        "        # logging.warning(f\"Standardization failed for a molecule: {e}\")\n",
        "        return None # Возвращаем None при любой ошибке стандартизации\n",
        "\n",
        "def calculate_all_features(smiles: str, descriptor_list: list) -> tuple[Optional[np.ndarray], Optional[list]]:\n",
        "    \"\"\"\n",
        "    Рассчитывает RDKit дескрипторы, ECFP и MACCS для одного SMILES.\n",
        "    Возвращает numpy массив признаков и список их имен.\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    std_mol = standardize_mol(mol) # Стандартизируем\n",
        "\n",
        "    if std_mol is None:\n",
        "        # logging.warning(f\"Could not process or standardize SMILES: {smiles}\")\n",
        "        return None, None # Возвращаем None, если молекула невалидна или не стандартизируется\n",
        "\n",
        "    feature_values = []\n",
        "    feature_names = []\n",
        "\n",
        "    # 1. RDKit Descriptors\n",
        "    for desc_name, desc_func in descriptor_list:\n",
        "        try:\n",
        "            val = desc_func(std_mol)\n",
        "            # Заменяем бесконечности и не-числа на NaN\n",
        "            if isinstance(val, (int, float)) and np.isfinite(val):\n",
        "                feature_values.append(float(val))\n",
        "            else:\n",
        "                feature_values.append(np.nan)\n",
        "        except Exception:\n",
        "            feature_values.append(np.nan)\n",
        "        feature_names.append(desc_name)\n",
        "\n",
        "    # 2. ECFP (Morgan) Fingerprint\n",
        "    ecfp_features = np.zeros((ECFP_NBITS,), dtype=np.int8) # Используем int8 для экономии памяти\n",
        "    try:\n",
        "        # fp = AllChem.GetMorganFingerprintAsBitVect(std_mol, ECFP_RADIUS, nBits=ECFP_NBITS)\n",
        "        fp = FP_GENERATOR.GetFingerprintAsNumPy(std_mol) # Используем генератор\n",
        "        # DataStructs.ConvertToNumpyArray(fp, ecfp_features) # Старый способ\n",
        "        ecfp_features = fp.astype(np.int8)\n",
        "    except Exception as e:\n",
        "        # logging.warning(f\"ECFP calculation failed for SMILES: {smiles} - {e}\")\n",
        "        pass # Оставляем нулевой вектор\n",
        "    feature_values.extend(ecfp_features.tolist())\n",
        "    feature_names.extend([f'ECFP_{i}' for i in range(ECFP_NBITS)])\n",
        "\n",
        "    # 3. MACCS Keys Fingerprint\n",
        "    maccs_features = np.zeros((167,), dtype=np.int8) # MACCS всегда 167 бит\n",
        "    try:\n",
        "        maccs_fp = AllChem.GetMACCSKeysFingerprint(std_mol)\n",
        "        # DataStructs.ConvertToNumpyArray(maccs_fp, maccs_features) # Старый способ\n",
        "        # Новый способ: прямое преобразование\n",
        "        for i in range(167):\n",
        "             if maccs_fp.GetBit(i):\n",
        "                 maccs_features[i] = 1\n",
        "    except Exception as e:\n",
        "        # logging.warning(f\"MACCS calculation failed for SMILES: {smiles} - {e}\")\n",
        "        pass # Оставляем нулевой вектор\n",
        "    feature_values.extend(maccs_features.tolist())\n",
        "    feature_names.extend([f'MACCS_{i}' for i in range(1, 168)]) # Нумерация с 1 до 167\n",
        "\n",
        "    return np.array(feature_values, dtype=np.float32), feature_names\n",
        "\n",
        "\n",
        "# --- 2. Основная Обработка ---\n",
        "\n",
        "# Получаем список RDKit дескрипторов один раз\n",
        "rdkit_descriptor_list = Descriptors.descList\n",
        "logging.info(f\"Using {len(rdkit_descriptor_list)} RDKit descriptors.\")\n",
        "\n",
        "# Определяем общую длину вектора признаков (приблизительно)\n",
        "# Это нужно для создания массива NaN в случае ошибки\n",
        "num_rdkit_desc = len(rdkit_descriptor_list)\n",
        "num_ecfp_bits = ECFP_NBITS\n",
        "num_maccs_bits = 167\n",
        "total_feature_length = num_rdkit_desc + num_ecfp_bits + num_maccs_bits\n",
        "logging.info(f\"Total expected feature length: {total_feature_length}\")\n",
        "\n",
        "all_feature_names = None # Определим по первому успешному расчету\n",
        "\n",
        "# --- Обработка Тренировочного Набора ---\n",
        "logging.info(f\"Processing Training Data from: {SOURCE_TRAIN_PATH}...\")\n",
        "try:\n",
        "    df_train_source = pd.read_csv(SOURCE_TRAIN_PATH)\n",
        "    if SMILES_COL not in df_train_source.columns or TARGET_COL not in df_train_source.columns:\n",
        "        raise ValueError(f\"Missing '{SMILES_COL}' or '{TARGET_COL}' in {SOURCE_TRAIN_PATH}\")\n",
        "    df_train_source.dropna(subset=[SMILES_COL, TARGET_COL], inplace=True)\n",
        "    logging.info(f\"Loaded {len(df_train_source)} training SMILES.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to load training data: {e}\"); sys.exit(1)\n",
        "\n",
        "train_features_list = []\n",
        "train_logp_list = []\n",
        "failed_train_smiles = 0\n",
        "\n",
        "pbar_train = tqdm_func(df_train_source.iterrows(), total=len(df_train_source), desc=\"Calculating Train Features\")\n",
        "for index, row in pbar_train:\n",
        "    smiles = row[SMILES_COL]\n",
        "    logp = row[TARGET_COL]\n",
        "    features, current_names = calculate_all_features(smiles, rdkit_descriptor_list)\n",
        "\n",
        "    if features is not None:\n",
        "        if all_feature_names is None: # Получаем имена фичей при первой удаче\n",
        "            all_feature_names = current_names\n",
        "            # Перепроверяем длину\n",
        "            if len(all_feature_names) != total_feature_length:\n",
        "                 logging.warning(f\"Feature name length ({len(all_feature_names)}) mismatch with expected ({total_feature_length}). Using actual.\")\n",
        "                 total_feature_length = len(all_feature_names)\n",
        "\n",
        "        # Проверяем консистентность длины вектора\n",
        "        if len(features) == total_feature_length:\n",
        "            train_features_list.append(features)\n",
        "            train_logp_list.append(logp)\n",
        "        else:\n",
        "            logging.warning(f\"Feature length mismatch for train SMILES {smiles} (got {len(features)}, expected {total_feature_length}). Skipping.\")\n",
        "            failed_train_smiles += 1\n",
        "    else:\n",
        "        failed_train_smiles += 1\n",
        "\n",
        "logging.info(f\"Finished processing train data. Success: {len(train_features_list)}, Failed: {failed_train_smiles}\")\n",
        "if not train_features_list:\n",
        "    logging.error(\"No features were successfully calculated for the training set. Exiting.\"); sys.exit(1)\n",
        "\n",
        "# Создаем DataFrame для трейна\n",
        "X_train_ext = pd.DataFrame(train_features_list, columns=all_feature_names)\n",
        "y_train_ext = pd.Series(train_logp_list, name=TARGET_COL)\n",
        "\n",
        "\n",
        "# --- Обработка Тестового Набора ---\n",
        "logging.info(f\"Processing Test Data from: {SOURCE_TEST_PATH}...\")\n",
        "try:\n",
        "    df_test_source = pd.read_csv(SOURCE_TEST_PATH)\n",
        "    if SMILES_COL not in df_test_source.columns:\n",
        "        raise ValueError(f\"Missing '{SMILES_COL}' in {SOURCE_TEST_PATH}\")\n",
        "    # Проверяем наличие ID, если нет - используем sample submission\n",
        "    if ID_COL not in df_test_source.columns:\n",
        "         logging.warning(f\"'{ID_COL}' not found in test data. Loading IDs from {SAMPLE_SUBMISSION_PATH}...\")\n",
        "         df_sample_sub = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
        "         if ID_COL not in df_sample_sub.columns: raise ValueError(f\"'{ID_COL}' not found in sample submission either!\")\n",
        "         if len(df_sample_sub) != len(df_test_source): raise ValueError(\"Length mismatch between test data and sample submission.\")\n",
        "         test_ids = df_sample_sub[ID_COL].values\n",
        "         df_test_source[ID_COL] = test_ids # Добавляем ID\n",
        "    else:\n",
        "         test_ids = df_test_source[ID_COL].values # Используем ID из test_data\n",
        "\n",
        "    df_test_source.dropna(subset=[SMILES_COL], inplace=True) # Удаляем только если SMILES пустой\n",
        "    logging.info(f\"Loaded {len(df_test_source)} test SMILES.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to load test data: {e}\"); sys.exit(1)\n",
        "\n",
        "test_features_list = []\n",
        "test_ids_list = [] # Сохраняем ID для строк, где фичи посчитались\n",
        "failed_test_smiles = 0\n",
        "fallback_feature_vector = np.full(total_feature_length, np.nan, dtype=np.float32) # Вектор для ошибок\n",
        "\n",
        "pbar_test = tqdm_func(df_test_source.iterrows(), total=len(df_test_source), desc=\"Calculating Test Features\")\n",
        "for index, row in pbar_test:\n",
        "    smiles = row[SMILES_COL]\n",
        "    current_id = row[ID_COL]\n",
        "    features, _ = calculate_all_features(smiles, rdkit_descriptor_list)\n",
        "\n",
        "    if features is not None and len(features) == total_feature_length:\n",
        "        test_features_list.append(features)\n",
        "    else:\n",
        "        # logging.warning(f\"Feature calculation failed or length mismatch for test SMILES {smiles}. Using NaN vector.\")\n",
        "        test_features_list.append(fallback_feature_vector) # Добавляем NaN вектор\n",
        "        failed_test_smiles += 1\n",
        "    test_ids_list.append(current_id) # Добавляем ID в любом случае, чтобы сохранить порядок\n",
        "\n",
        "logging.info(f\"Finished processing test data. Success/Fallback: {len(test_features_list)}, Failed to generate vector (should be 0): {failed_test_smiles}\")\n",
        "if not test_features_list:\n",
        "    logging.error(\"No features were calculated for the test set. Exiting.\"); sys.exit(1)\n",
        "\n",
        "# Создаем DataFrame для теста\n",
        "X_test_ext = pd.DataFrame(test_features_list, columns=all_feature_names)\n",
        "\n",
        "\n",
        "# --- 3. Импутация NaN ---\n",
        "logging.info(\"Handling NaN values with Median Imputation...\")\n",
        "nan_counts_train = X_train_ext.isnull().sum()\n",
        "cols_with_nan_train = nan_counts_train[nan_counts_train > 0]\n",
        "if not cols_with_nan_train.empty:\n",
        "    logging.info(f\"NaNs found in {len(cols_with_nan_train)} training features columns.\")\n",
        "    # print(cols_with_nan_train) # Можно раскомментировать для детального просмотра\n",
        "\n",
        "    imputer = SimpleImputer(strategy='median') # Медиана более устойчива к выбросам\n",
        "\n",
        "    logging.info(\"Fitting imputer on training data...\")\n",
        "    X_train_imputed = imputer.fit_transform(X_train_ext)\n",
        "    X_train_ext = pd.DataFrame(X_train_imputed, columns=all_feature_names, index=X_train_ext.index)\n",
        "    logging.info(\"Training data imputed.\")\n",
        "\n",
        "    logging.info(\"Transforming test data with fitted imputer...\")\n",
        "    nan_counts_test_before = X_test_ext.isnull().sum().sum()\n",
        "    if nan_counts_test_before > 0:\n",
        "         X_test_imputed = imputer.transform(X_test_ext)\n",
        "         X_test_ext = pd.DataFrame(X_test_imputed, columns=all_feature_names, index=X_test_ext.index)\n",
        "         logging.info(f\"Test data imputed. {nan_counts_test_before} NaNs filled.\")\n",
        "    else:\n",
        "         logging.info(\"No NaNs found in test data before imputation.\")\n",
        "\n",
        "    # Сохраняем импутер\n",
        "    joblib.dump(imputer, OUTPUT_IMPUTER_PATH)\n",
        "    logging.info(f\"Imputer saved to {OUTPUT_IMPUTER_PATH}\")\n",
        "\n",
        "else:\n",
        "    logging.info(\"No NaN values found in training features. Skipping imputation.\")\n",
        "    # Проверяем NaN в тесте на всякий случай\n",
        "    nan_counts_test = X_test_ext.isnull().sum().sum()\n",
        "    if nan_counts_test > 0:\n",
        "         logging.warning(f\"NaNs found in TEST data ({nan_counts_test}) but not in TRAIN data. Filling with 0.\")\n",
        "         X_test_ext.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# --- 4. Сохранение Результатов ---\n",
        "\n",
        "# Собираем финальные датафреймы\n",
        "final_train_df = pd.concat([X_train_ext, y_train_ext], axis=1)\n",
        "final_test_df = X_test_ext.copy()\n",
        "final_test_df[ID_COL] = test_ids_list # Добавляем ID\n",
        "\n",
        "# Перемещаем колонку ID в начало тестового DataFrame\n",
        "cols = [ID_COL] + [col for col in final_test_df.columns if col != ID_COL]\n",
        "final_test_df = final_test_df[cols]\n",
        "\n",
        "logging.info(f\"Final Train DataFrame shape: {final_train_df.shape}\")\n",
        "logging.info(f\"Final Test DataFrame shape: {final_test_df.shape}\")\n",
        "\n",
        "logging.info(f\"Saving extended training features to: {OUTPUT_TRAIN_EXTENDED_PATH}\")\n",
        "final_train_df.to_csv(OUTPUT_TRAIN_EXTENDED_PATH, index=False)\n",
        "\n",
        "logging.info(f\"Saving extended test features to: {OUTPUT_TEST_EXTENDED_PATH}\")\n",
        "final_test_df.to_csv(OUTPUT_TEST_EXTENDED_PATH, index=False)\n",
        "\n",
        "logging.info(\"--- Feature Engineering Script Finished Successfully ---\")"
      ],
      "metadata": {
        "id": "iWbgeQUm7Wz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "TRAIN_DATA_PATH = 'train_features_extended.csv'         # <<< Расширенные признаки трейна\n",
        "TEST_DATA_PATH = 'test_features_extended.csv'           # <<< Расширенные признаки теста\n",
        "SAMPLE_SUBMISSION_PATH = 'final_sample_submission80.csv' # Пример сабмишена\n",
        "\n",
        "TARGET_COLUMN = 'LogP'\n",
        "ID_COLUMN = 'ID' # ID должен быть в test_features_extended.csv\n",
        "RANDOM_STATE = 42\n",
        "CV_FOLDS = 5 # 5 или 10\n",
        "N_TRIALS = 200 # Количество итераций Optuna (увеличь при наличии времени, >150)\n",
        "OPTUNA_TIMEOUT = None # В секундах, None - без таймаута\n",
        "SCORING = 'neg_root_mean_squared_error' # Цель Optuna - минимизация RMSE\n",
        "OPTUNA_DIRECTION = \"maximize\" # Максимизируем neg_rmse\n",
        "VT_THRESHOLD = 0.001 # Начальный порог для VarianceThreshold\n",
        "SFM_THRESHOLD = 'median' # Порог для SelectFromModel ('mean', 'median', или число, например, 1e-5)\n",
        "\n",
        "\n",
        "OUTPUT_DIR = Path('./lgbm_extended_fs_output')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "OPTUNA_DB_PATH = f\"sqlite:///{OUTPUT_DIR}/optuna_lgbm_ext_fs.db\"\n",
        "STUDY_NAME = \"lgbm_ext_fs_optimization_v1\"\n",
        "OUTPUT_MODEL_PATH = OUTPUT_DIR / 'final_lgbm_ext_fs_pipeline.pkl'\n",
        "OUTPUT_SUBMISSION_PATH = OUTPUT_DIR / 'submission_lgbm_ext_fs.csv'\n",
        "SELECTED_FEATURES_PATH = OUTPUT_DIR / 'selected_features.txt' # Сохраним список отобранных фичей\n",
        "\n",
        "\n",
        "\n",
        "def load_data(file_path: str, target_col: Optional[str] = None):\n",
        "    \"\"\"Загружает данные.\"\"\"\n",
        "    logging.info(f\"Загрузка данных из {file_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        logging.info(f\"Загружен файл: {file_path}, форма: {df.shape}\")\n",
        "        if target_col and target_col not in df.columns:\n",
        "            raise ValueError(f\"Целевая колонка '{target_col}' не найдена.\")\n",
        "        if ID_COLUMN not in df.columns and target_col is None: # Проверка ID в тестовом файле\n",
        "             raise ValueError(f\"Колонка ID '{ID_COLUMN}' не найдена в тестовом файле {file_path}.\")\n",
        "        return df\n",
        "    except FileNotFoundError: logging.error(f\"Ошибка: Файл '{file_path}' не найден.\"); sys.exit(1)\n",
        "    except ValueError as e: logging.error(f\"Ошибка данных: {e}\"); sys.exit(1)\n",
        "    except Exception as e: logging.error(f\"Ошибка загрузки файла {file_path}: {e}\"); sys.exit(1)\n",
        "\n",
        "def create_pipeline_optuna(trial: optuna.trial.Trial) -> Pipeline:\n",
        "    \"\"\"Создает пайплайн для Optuna (БЕЗ отбора признаков внутри).\"\"\"\n",
        "    # Параметры препроцессинга (если нужен)\n",
        "    scaler_type = trial.suggest_categorical(\"scaler\", [\"StandardScaler\", \"None\"])\n",
        "\n",
        "    preprocessing_steps = []\n",
        "    if scaler_type == \"StandardScaler\":\n",
        "        preprocessing_steps.append(('scaler', StandardScaler()))\n",
        "\n",
        "    # Параметры LightGBM\n",
        "    lgbm_params = {\n",
        "        'objective': 'regression_l1',\n",
        "        'metric': 'rmse',\n",
        "        'random_state': RANDOM_STATE,\n",
        "        'n_jobs': -1,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True), # Уменьшил верхнюю границу LR\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 3000), # Можно увеличить n_estimators\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 150), # Увеличил диапазон\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 20), # Увеличил диапазон\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-7, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-7, 10.0, log=True),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100), # Увеличил диапазон\n",
        "    }\n",
        "    model = lgb.LGBMRegressor(**lgbm_params)\n",
        "    pipeline = Pipeline(preprocessing_steps + [('lgbm', model)])\n",
        "    return pipeline\n",
        "\n",
        "def objective(trial: optuna.trial.Trial, X_tr: pd.DataFrame, y_tr: pd.Series, cv_folds: int, scoring: str) -> float:\n",
        "    \"\"\"Целевая функция для Optuna.\"\"\"\n",
        "    pipeline = create_pipeline_optuna(trial)\n",
        "    try:\n",
        "        cv = KFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE + trial.number) # Меняем random_state для CV\n",
        "        scores = cross_val_score(pipeline, X_tr, y_tr, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "        mean_score = np.mean(scores)\n",
        "        if np.isnan(mean_score) or np.isinf(mean_score): raise ValueError(\"CV score is NaN/inf\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Ошибка CV для триала {trial.number}: {e}\")\n",
        "        return -np.inf # Возвращаем худшее значение для максимизации\n",
        "    return mean_score\n",
        "\n",
        "def train_final_model(best_params: dict, X_train_full_sel: pd.DataFrame, y_train_full: pd.Series) -> Pipeline:\n",
        "    \"\"\"Обучает финальный пайплайн на ВСЕХ отобранных обучающих данных.\"\"\"\n",
        "    logging.info(\"--- Обучение финальной модели с лучшими параметрами ---\")\n",
        "    # Воссоздаем пайплайн препроцессинга\n",
        "    preprocessing_steps = []\n",
        "    scaler_type = best_params.get(\"scaler\", \"None\") # Дефолт None, если не было в HPO\n",
        "    if scaler_type == \"StandardScaler\":\n",
        "        preprocessing_steps.append(('scaler', StandardScaler()))\n",
        "        logging.info(\"Using StandardScaler in final model.\")\n",
        "    else:\n",
        "        logging.info(\"Scaler not used in final model.\")\n",
        "\n",
        "    # Воссоздаем параметры LightGBM\n",
        "    lgbm_params = {\n",
        "        'objective': 'regression_l1', 'metric': 'rmse', 'random_state': RANDOM_STATE, 'n_jobs': -1,\n",
        "        'learning_rate': best_params['learning_rate'], 'n_estimators': best_params['n_estimators'],\n",
        "        'num_leaves': best_params['num_leaves'], 'max_depth': best_params['max_depth'],\n",
        "        'reg_alpha': best_params['reg_alpha'], 'reg_lambda': best_params['reg_lambda'],\n",
        "        'colsample_bytree': best_params['colsample_bytree'], 'subsample': best_params['subsample'],\n",
        "        'min_child_samples': best_params['min_child_samples'],\n",
        "    }\n",
        "    final_model = lgb.LGBMRegressor(**lgbm_params)\n",
        "    final_pipeline = Pipeline(preprocessing_steps + [('lgbm', final_model)])\n",
        "\n",
        "    logging.info(\"Обучение на полном ОТОБРАННОМ обучающем датасете...\")\n",
        "    start_time = time.time()\n",
        "    final_pipeline.fit(X_train_full_sel, y_train_full)\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Финальная модель обучена за {end_time - start_time:.2f} секунд.\")\n",
        "    logging.info(f\"Количество признаков, использованных моделью: {X_train_full_sel.shape[1]}\")\n",
        "\n",
        "    # Вывод важности признаков\n",
        "    try:\n",
        "        lgbm_step = final_pipeline.named_steps['lgbm']\n",
        "        if hasattr(lgbm_step, 'feature_importances_'):\n",
        "            importances = lgbm_step.feature_importances_\n",
        "            feature_names_ = X_train_full_sel.columns.tolist() # Используем имена отобранных признаков\n",
        "            if len(importances) == len(feature_names_):\n",
        "                feature_importance_df = pd.DataFrame({'feature': feature_names_, 'importance': importances})\n",
        "                feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "                logging.info(\"\\nТоп-10 самых важных признаков (финальная модель):\")\n",
        "                logging.info(\"\\n\" + feature_importance_df.head(10).to_string())\n",
        "            else:\n",
        "                logging.warning(\"Несовпадение кол-ва важностей и признаков.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Не удалось получить/вывести важность признаков: {e}\")\n",
        "\n",
        "    return final_pipeline\n",
        "\n",
        "def evaluate_model(pipeline: Pipeline, X_test_sel: pd.DataFrame, y_test: pd.Series):\n",
        "    \"\"\"Оценивает пайплайн на ОТОБРАННЫХ тестовых данных.\"\"\"\n",
        "    logging.info(\"--- Оценка качества финальной модели на ЛОКАЛЬНОЙ тестовой выборке ---\")\n",
        "    try:\n",
        "        y_pred = pipeline.predict(X_test_sel)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        logging.info(f\"Mean Squared Error (MSE): {mse:.5f}\")\n",
        "        logging.info(f\"Root Mean Squared Error (RMSE): {rmse:.5f}\") # <<< ВАЖНАЯ МЕТРИКА\n",
        "        logging.info(f\"Coefficient of Determination (R²): {r2:.5f}\")\n",
        "        return y_pred, {'mse': mse, 'rmse': rmse, 'r2': r2}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Ошибка при оценке модели: {e}\")\n",
        "        return None, {}\n",
        "\n",
        "def create_submission(pipeline: Pipeline, test_data_path: str, selected_feature_names: list,\n",
        "                      sample_submission_path: str, output_path: Path, id_col: str, target_col: str):\n",
        "    \"\"\"Делает предсказания на тестовом наборе (ИСПОЛЬЗУЯ ОТОБРАННЫЕ ПРИЗНАКИ) и сохраняет файл.\"\"\"\n",
        "    logging.info(\"--- Создание файла для отправки ---\")\n",
        "    try:\n",
        "        test_df_full = pd.read_csv(test_data_path) # Загружаем ПОЛНЫЙ тестовый файл с РАСШИРЕННЫМИ признаками\n",
        "        logging.info(f\"Загружен полный тестовый набор: {test_df_full.shape}\")\n",
        "\n",
        "        # --- ВАЖНО: Отбираем только НУЖНЫЕ признаки ---\n",
        "        if not all(f in test_df_full.columns for f in selected_feature_names):\n",
        "             missing_cols = [f for f in selected_feature_names if f not in test_df_full.columns]\n",
        "             raise ValueError(f\"Не найдены все отобранные признаки в тестовом файле! Отсутствуют: {missing_cols}\")\n",
        "\n",
        "        X_test_submission_sel = test_df_full[selected_feature_names]\n",
        "        logging.info(f\"Отобраны признаки для предсказания: {X_test_submission_sel.shape}\")\n",
        "\n",
        "        # Проверка и обработка NaN (на всякий случай, хотя импутер должен был сработать)\n",
        "        if X_test_submission_sel.isnull().sum().sum() > 0:\n",
        "             logging.warning(\"Найдены NaN в отобранных тестовых признаках! Попытка заполнить нулями...\")\n",
        "             X_test_submission_sel = X_test_submission_sel.fillna(0) # Простая замена\n",
        "\n",
        "        logging.info(\"Генерация предсказаний...\")\n",
        "        predictions = pipeline.predict(X_test_submission_sel)\n",
        "\n",
        "        # Создание DataFrame для submission\n",
        "        submission_df = pd.DataFrame({ id_col: test_df_full[id_col], target_col: predictions })\n",
        "\n",
        "        # Сопоставление с Sample Submission для правильного порядка и набора ID\n",
        "        df_sample = pd.read_csv(sample_submission_path)[[id_col]]\n",
        "        final_submission_df = df_sample.merge(submission_df, on=id_col, how='left')\n",
        "\n",
        "        if final_submission_df[target_col].isnull().any():\n",
        "            logging.warning(\"NaN в итоговом сабмишене после merge! Проверьте ID.\")\n",
        "        if len(final_submission_df) != len(df_sample):\n",
        "             logging.warning(\"Размер итогового сабмишена не совпадает с sample submission!\")\n",
        "\n",
        "        final_submission_df[[id_col, target_col]].to_csv(output_path, index=False)\n",
        "        logging.info(f\"Файл с предсказаниями сохранен в: {output_path}\")\n",
        "        logging.info(\"Пример предсказаний:\\n\" + final_submission_df.head().to_string())\n",
        "\n",
        "    except FileNotFoundError: logging.error(f\"Ошибка: Файл не найден при создании сабмишена.\"); sys.exit(1)\n",
        "    except ValueError as e: logging.error(f\"Ошибка данных при создании сабмишена: {e}\"); sys.exit(1)\n",
        "    except Exception as e: logging.error(f\"Ошибка при создании файла submission: {e}\"); sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- 2.1 Загрузка Полных Данных ---\n",
        "    df_train_ext = load_data(TRAIN_DATA_PATH, target_col=TARGET_COLUMN)\n",
        "    df_test_ext = load_data(TEST_DATA_PATH) # ID должен быть там\n",
        "\n",
        "    X_full_train = df_train_ext.drop(columns=[TARGET_COLUMN])\n",
        "    y_full_train = df_train_ext[TARGET_COLUMN]\n",
        "    X_full_test  = df_test_ext.drop(columns=[ID_COLUMN]) # Убираем ID для обработки\n",
        "    test_ids     = df_test_ext[ID_COLUMN].values         # Сохраняем ID\n",
        "\n",
        "    # Проверка совпадения колонок (кроме таргета/ID)\n",
        "    train_cols = set(X_full_train.columns)\n",
        "    test_cols = set(X_full_test.columns)\n",
        "    if train_cols != test_cols:\n",
        "        logging.error(\"Колонки в трейне и тесте (расширенные признаки) не совпадают!\")\n",
        "        logging.error(f\"Только в трейне: {train_cols - test_cols}\")\n",
        "        logging.error(f\"Только в тесте: {test_cols - train_cols}\")\n",
        "        sys.exit(1)\n",
        "    feature_names_initial = list(X_full_train.columns) # Имена всех исходных расширенных признаков\n",
        "\n",
        "    # --- 2.2 Отбор Признаков ---\n",
        "    logging.info(\"--- Выполнение отбора признаков ---\")\n",
        "    start_fs_time = time.time()\n",
        "\n",
        "    # Шаг 1: Variance Threshold\n",
        "    logging.info(f\"Применение VarianceThreshold (threshold={VT_THRESHOLD})...\")\n",
        "    vt = VarianceThreshold(threshold=VT_THRESHOLD)\n",
        "    try:\n",
        "        X_train_vt = vt.fit_transform(X_full_train)\n",
        "        X_test_vt = vt.transform(X_full_test)\n",
        "        selected_features_vt_mask = vt.get_support()\n",
        "        selected_feature_names_vt = [name for i, name in enumerate(feature_names_initial) if selected_features_vt_mask[i]]\n",
        "        num_removed_vt = len(feature_names_initial) - len(selected_feature_names_vt)\n",
        "        logging.info(f\"VarianceThreshold удалил {num_removed_vt} признаков.\")\n",
        "        if not selected_feature_names_vt: raise ValueError(\"VarianceThreshold удалил все признаки!\")\n",
        "        # Обновляем DataFrame'ы\n",
        "        X_train_processed = pd.DataFrame(X_train_vt, columns=selected_feature_names_vt, index=X_full_train.index)\n",
        "        X_test_processed = pd.DataFrame(X_test_vt, columns=selected_feature_names_vt, index=X_full_test.index)\n",
        "        logging.info(f\"Признаков после VarianceThreshold: {X_train_processed.shape[1]}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Ошибка на этапе VarianceThreshold: {e}\"); sys.exit(1)\n",
        "\n",
        "    # Шаг 2: SelectFromModel (LGBM)\n",
        "    logging.info(f\"Применение SelectFromModel(LGBM) (threshold={SFM_THRESHOLD})...\")\n",
        "    try:\n",
        "        # Используем простые параметры для модели-селектора\n",
        "        selector_model = lgb.LGBMRegressor(\n",
        "            objective='regression_l1', random_state=RANDOM_STATE, n_jobs=-1,\n",
        "            n_estimators=200, learning_rate=0.05, num_leaves=31 # Простые параметры\n",
        "        )\n",
        "        selector = SelectFromModel(\n",
        "            selector_model,\n",
        "            threshold=SFM_THRESHOLD,\n",
        "            # max_features=SFM_MAX_FEATURES, # Используй ИЛИ threshold ИЛИ max_features\n",
        "            prefit=False\n",
        "        )\n",
        "        selector.fit(X_train_processed, y_full_train) # Обучаем селектор\n",
        "        selected_features_final_mask = selector.get_support()\n",
        "        selected_feature_names_final = X_train_processed.columns[selected_features_final_mask].tolist()\n",
        "        num_selected_sfm = len(selected_feature_names_final)\n",
        "        logging.info(f\"SelectFromModel отобрал {num_selected_sfm} признаков.\")\n",
        "        if not selected_feature_names_final: raise ValueError(\"SelectFromModel не отобрал ни одного признака!\")\n",
        "\n",
        "        # --- Финальные отобранные признаки ---\n",
        "        X_train_selected = X_train_processed[selected_feature_names_final]\n",
        "        X_test_selected = X_test_processed[selected_feature_names_final] # Отбираем те же признаки из теста\n",
        "\n",
        "        # Сохраняем список отобранных признаков\n",
        "        with open(SELECTED_FEATURES_PATH, 'w') as f:\n",
        "            for feature_name in selected_feature_names_final:\n",
        "                f.write(f\"{feature_name}\\n\")\n",
        "        logging.info(f\"Список отобранных признаков сохранен в: {SELECTED_FEATURES_PATH}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Ошибка на этапе SelectFromModel: {e}\"); sys.exit(1)\n",
        "\n",
        "    logging.info(f\"Отбор признаков завершен за {time.time() - start_fs_time:.2f} секунд.\")\n",
        "    logging.info(f\"Итоговое количество признаков для моделирования: {X_train_selected.shape[1]}\")\n",
        "\n",
        "    # --- 2.3 Локальный Train/Test Split (на отобранных признаках) ---\n",
        "    logging.info(\"Создание локального train/test split для оценки...\")\n",
        "    X_train_local, X_test_local, y_train_local, y_test_local = train_test_split(\n",
        "        X_train_selected, # Используем отобранные!\n",
        "        y_full_train,     # Соответствующий таргет\n",
        "        test_size=0.2,    # Стандартный размер для локальной оценки\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "    logging.info(f\"Размеры локальных выборок: X_train={X_train_local.shape}, X_test={X_test_local.shape}\")\n",
        "\n",
        "    # --- 2.4 Запуск Optuna HPO ---\n",
        "    logging.info(\"--- Запуск оптимизации Optuna ---\")\n",
        "    study = optuna.create_study(\n",
        "        study_name=STUDY_NAME,\n",
        "        storage=OPTUNA_DB_PATH,\n",
        "        direction=OPTUNA_DIRECTION,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        load_if_exists=True # Позволяет продолжить оптимизацию\n",
        "    )\n",
        "\n",
        "    # Определяем, сколько триалов осталось запустить\n",
        "    completed_trials = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
        "    trials_to_run = N_TRIALS - completed_trials\n",
        "    logging.info(f\"Всего задано триалов: {N_TRIALS}, уже завершено: {completed_trials}\")\n",
        "\n",
        "    if trials_to_run > 0:\n",
        "        logging.info(f\"Запускаем {trials_to_run} новых триалов...\")\n",
        "        start_optuna_time = time.time()\n",
        "        try:\n",
        "            study.optimize(\n",
        "                lambda trial: objective(trial, X_train_local, y_train_local, CV_FOLDS, SCORING), # Оптимизация на локальном трейне\n",
        "                n_trials=trials_to_run,\n",
        "                timeout=OPTUNA_TIMEOUT,\n",
        "                n_jobs=1, # Optuna в 1 поток, CV распараллеливается внутри\n",
        "                show_progress_bar=True\n",
        "            )\n",
        "            logging.info(f\"Сессия оптимизации Optuna завершена за {time.time() - start_optuna_time:.2f} секунд.\")\n",
        "        except KeyboardInterrupt: logging.info(\"Оптимизация прервана пользователем.\")\n",
        "        except Exception as e: logging.error(f\"Ошибка во время оптимизации Optuna: {e}\")\n",
        "    else:\n",
        "        logging.info(\"Заданное количество триалов Optuna уже выполнено.\")\n",
        "\n",
        "    # --- 2.5 Обучение и Оценка Финальной Модели ---\n",
        "    if study.best_trial:\n",
        "        logging.info(\"--- Результаты оптимизации Optuna ---\")\n",
        "        best_params = study.best_params\n",
        "        best_cv_score = study.best_value\n",
        "        best_cv_rmse = np.sqrt(-best_cv_score) if SCORING == 'neg_root_mean_squared_error' else best_cv_score\n",
        "        logging.info(f\"Лучшее значение RMSE на CV (валидация Optuna): {best_cv_rmse:.5f}\")\n",
        "        logging.info(\"Лучшие гиперпараметры:\")\n",
        "        for key, value in best_params.items(): logging.info(f\"  {key}: {value}\")\n",
        "\n",
        "        # Обучаем финальную модель на ВСЕХ отобранных трейн данных\n",
        "        final_pipeline = train_final_model(best_params, X_train_selected, y_full_train)\n",
        "\n",
        "        # Сохраняем финальную модель\n",
        "        try:\n",
        "            joblib.dump(final_pipeline, OUTPUT_MODEL_PATH)\n",
        "            logging.info(f\"Финальный пайплайн сохранен в: {OUTPUT_MODEL_PATH}\")\n",
        "        except Exception as e: logging.error(f\"Ошибка при сохранении финальной модели: {e}\")\n",
        "\n",
        "        # Оцениваем на локальном ОТОБРАННОМ тесте\n",
        "        evaluate_model(final_pipeline, X_test_local, y_test_local)\n",
        "\n",
        "        # --- 2.6 Создание Финального Сабмишена ---\n",
        "        create_submission(\n",
        "            pipeline=final_pipeline,\n",
        "            test_data_path=TEST_DATA_PATH, # Путь к ПОЛНОМУ тестовому файлу с РАСШИРЕННЫМИ признаками\n",
        "            selected_feature_names=selected_feature_names_final, # Список ОТОБРАННЫХ имен\n",
        "            sample_submission_path=SAMPLE_SUBMISSION_PATH,\n",
        "            output_path=OUTPUT_SUBMISSION_PATH,\n",
        "            id_col=ID_COLUMN,\n",
        "            target_col=TARGET_COLUMN\n",
        "        )\n",
        "    else:\n",
        "        logging.warning(\"Optuna не нашла лучшего триала. Финальная модель не обучена, сабмишен не создан.\")\n",
        "\n",
        "    logging.info(\"--- СКРИПТ HPO LGBM (Extended + FS) ЗАВЕРШЕН ---\")"
      ],
      "metadata": {
        "id": "KweTWt8z8-G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Остановка скрипта вручную.\n",
        "- Результат продолжительное время не улучшался.\n",
        "- Оставшееся время оценивалось предположительно в 70 часов"
      ],
      "metadata": {
        "id": "cszxaO47MrG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "uAu7DIZ43Lnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### СТЕКИНГ (SVM + LGBM + GNN)"
      ],
      "metadata": {
        "id": "PpWxBlwl5Q6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# --- ЕДИНЫЙ СКРИПТ ДЛЯ СТЕКИНГА (SVM + LGBM + GNN) v2 ---\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "import joblib\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Визуализация (опционально)\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# RDKit\n",
        "from rdkit import Chem, RDLogger\n",
        "from rdkit.Chem import Descriptors, AllChem, rdPartialCharges\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge # Мета-модель\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# LightGBM\n",
        "import lightgbm as lgb\n",
        "\n",
        "# PyTorch & PyG (для GNN)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, InMemoryDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GINEConv, GATv2Conv, global_add_pool, global_mean_pool, global_max_pool\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "# Optuna (для мета-модели)\n",
        "import optuna\n",
        "\n",
        "# Утилиты\n",
        "# Определяем tqdm в зависимости от среды\n",
        "try:\n",
        "    from tqdm.notebook import tqdm as tqdm_notebook\n",
        "    tqdm_func = tqdm_notebook\n",
        "    print(\"Using tqdm.notebook\")\n",
        "except ImportError:\n",
        "    from tqdm import tqdm as tqdm_base\n",
        "    tqdm_func = tqdm_base\n",
        "    print(\"Using standard tqdm\")\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import sys # Для sys.exit()\n",
        "\n",
        "# --- 0. Настройка и Конфигурация ---\n",
        "# Игнорирование предупреждений\n",
        "warnings.filterwarnings('ignore', category=UserWarning, message='.*X does not have valid feature names.*')\n",
        "# warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"TypedStorage is deprecated\")\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Основные пути к файлам (!!! ПРОВЕРЬ ИХ !!!)\n",
        "SOURCE_TRAIN_DATA_PATH = 'train_data_clean_rdkit (2).csv' # Файл с SMILES, LogP (БЕЗ ID)\n",
        "SOURCE_TEST_DATA_PATH = 'test_data.csv'                 # Файл с SMILES (и ID?, проверим) для теста\n",
        "SAMPLE_SUBMISSION_PATH = 'final_sample_submission80.csv' # Пример сабмишена (с ID)\n",
        "\n",
        "SVM_LGBM_TRAIN_FEATURES_PATH = 'final_train_data_04_14_201_new_alex.csv' # 201 признак + LogP для трейна\n",
        "SVM_LGBM_TEST_FEATURES_PATH = 'final_test_data_04_14_201_new_alex.csv'  # 201 признак для теста\n",
        "\n",
        "# Параметры\n",
        "TARGET_COLUMN = 'LogP'\n",
        "SMILES_COLUMN = 'SMILES'\n",
        "ID_COLUMN = 'ID' # Будем генерировать эту колонку, если ее нет\n",
        "SPLIT_RANDOM_STATE = 42\n",
        "VALIDATION_SET_SIZE = 0.2 # Доля данных для валидации (OOF)\n",
        "\n",
        "# Настройки GNN\n",
        "GNN_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.info(f\"GNN using device: {GNN_DEVICE}\")\n",
        "GNN_BATCH_SIZE = 64\n",
        "GNN_EPOCHS = 115\n",
        "GNN_FEATURE_DIMS = (16, 9, 219)\n",
        "\n",
        "# Настройки Мета-модели (Ridge + Optuna)\n",
        "META_MODEL_CV_FOLDS = 5\n",
        "META_MODEL_OPTUNA_TRIALS = 50\n",
        "META_MODEL_OPTUNA_TIMEOUT = 300 # Секунды\n",
        "\n",
        "# Пути для сохранения промежуточных и финальных результатов\n",
        "OUTPUT_DIR = Path('./stacking_output_v2') # Используем v2 для новой версии\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "# --- 1. Лучшие Гиперпараметры Базовых Моделей ---\n",
        "# (Оставляем без изменений)\n",
        "BEST_SVM_PARAMS = { 'variance_threshold': 0.017612840487812365, 'kernel': 'rbf', 'C': 2.772897695046977, 'gamma_rbf': 'scale' }\n",
        "BEST_LGBM_PARAMS = { 'scaler': 'None', 'use_variance_threshold': True, 'variance_threshold': 0.014620133713184701, 'learning_rate': 0.04287683192058186, 'n_estimators': 1954, 'num_leaves': 95, 'max_depth': 11, 'reg_alpha': 2.0709243779799007e-07, 'reg_lambda': 4.906962698572223e-06, 'colsample_bytree': 0.5094276601924259, 'subsample': 0.5613723967905625, 'min_child_samples': 5, 'objective': 'regression_l1', 'metric': 'rmse', 'random_state': SPLIT_RANDOM_STATE, 'n_jobs': -1 }\n",
        "BEST_HYPERPARAMS_GNN = { 'gin_hidden': 128, 'n_layers': 5, 'desc_hidden': 1024, 'dropout': 0.45, 'activation': 'gelu', 'pooling': 'add', 'gnn_type': 'GINE', 'gat_heads': 4, 'train_eps': True, 'final_mlp_layers': 2, 'norm_type': 'LayerNorm', 'clip_grad_norm': 1.0, 'lr': 0.000518107585097511, 'weight_decay': 0.0004637649821784796, 'optimizer': 'AdamW', 'loss_delta': 1.3507041013870287, 'scheduler_T0': 12, 'scheduler_T_mult': 2 }\n",
        "\n",
        "\n",
        "# --- 2. Классы и Функции GNN (С ИСПРАВЛЕНИЕМ SMARTS) ---\n",
        "\n",
        "# Функция вычисления дескрипторов v2\n",
        "def compute_descriptors_v2(smiles: str) -> tuple[Optional[np.ndarray], Optional[list]]:\n",
        "    # (Без изменений)\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None: return None, None\n",
        "        all_desc_values = []; desc_names = []\n",
        "        for desc_name, desc_func in Descriptors.descList:\n",
        "            try: val = desc_func(mol); all_desc_values.append(float(val) if isinstance(val, (int, float)) and not (np.isnan(val) or np.isinf(val)) else np.nan)\n",
        "            except Exception: all_desc_values.append(np.nan)\n",
        "            desc_names.append(desc_name)\n",
        "        try:\n",
        "            rdPartialCharges.ComputeGasteigerCharges(mol); all_desc_values.append(Descriptors.MaxAbsPartialCharge(mol)); desc_names.append(\"MaxAbsPartialCharge\"); all_desc_values.append(Descriptors.MinAbsPartialCharge(mol)); desc_names.append(\"MinAbsPartialCharge\")\n",
        "        except Exception: all_desc_values.extend([np.nan, np.nan]); desc_names.extend([\"MaxAbsPartialCharge\", \"MinAbsPartialCharge\"])\n",
        "        descriptors = np.array(all_desc_values, dtype=np.float32)\n",
        "        return descriptors, desc_names\n",
        "    except Exception: return None, None\n",
        "\n",
        "class LogPInMemoryDataset(InMemoryDataset):\n",
        "    # SMARTS компиляция (Без изменений)\n",
        "    _acid_query: Optional[Chem.Mol] = None\n",
        "    _base_query: Optional[Chem.Mol] = None\n",
        "    _smarts_compiled = False\n",
        "    try:\n",
        "        # ... (код компиляции SMARTS) ...\n",
        "        _acid_smarts = '[CX3](=O)[OX1H0-,OX2H1]'\n",
        "        _base_smarts = '[#7+0;!$(*~[#6]=[#7,#8,#16]);!$(*~[#16]=[#8]);!$(*~[#7]=[#8])],[#7+1X4]'\n",
        "        _acid_query = Chem.MolFromSmarts(_acid_smarts)\n",
        "        _base_query = Chem.MolFromSmarts(_base_smarts)\n",
        "        if _acid_query is not None and _base_query is not None:\n",
        "             _smarts_compiled = True\n",
        "             # logging.info(\"GNN Dataset: Charge/Zwitterion SMARTS patterns compiled successfully.\")\n",
        "        else: _smarts_compiled = False; logging.error(\"GNN Dataset: Failed to compile SMARTS.\")\n",
        "    except Exception as e: logging.error(f\"GNN Dataset: Unexpected error compiling SMARTS: {e}.\"); _smarts_compiled = False\n",
        "\n",
        "    def __init__(self, root, smiles_list, logp_list, fit_scaler=False, scaler=None,\n",
        "                 transform=None, pre_transform=None, pre_filter=None):\n",
        "\n",
        "        self.root_path = Path(root)\n",
        "        # <<< ИЗМЕНЕНИЕ: Устанавливаем атрибуты ДО вызова super() >>>\n",
        "        self.smiles_list = smiles_list\n",
        "        self.logp_list = logp_list\n",
        "        self.fit_scaler = fit_scaler\n",
        "        # Если scaler не передан, создаем новый. Он будет обучен в process(), если fit_scaler=True\n",
        "        self.scaler = scaler if scaler is not None else StandardScaler()\n",
        "        self._processed_feature_dims = None # Инициализируем здесь\n",
        "\n",
        "        # Удаляем старые данные, если они есть, ДО вызова super(), чтобы process() точно запустился\n",
        "        if self.root_path.exists() and not self.processed_paths_exist():\n",
        "            logging.warning(f\"GNN Dataset: Root exists but processed files not found. Removing {self.root_path}\")\n",
        "            shutil.rmtree(self.root_path)\n",
        "        elif self.root_path.exists() and self.processed_paths_exist():\n",
        "             logging.info(f\"GNN Dataset: Processed files found in {self.root_path}. Skipping process().\")\n",
        "\n",
        "\n",
        "        # <<< ИЗМЕНЕНИЕ: super() вызывается ПОСЛЕ установки атрибутов >>>\n",
        "        super().__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "        # <<< ИЗМЕНЕНИЕ: Загружаем данные только если process() НЕ был вызван >>>\n",
        "        # process() вызывается внутри super().__init__(), если файлы не найдены.\n",
        "        # Если файлы были найдены, super() их не трогает, и нам нужно загрузить их здесь.\n",
        "        if not hasattr(self, 'data') or self.data is None: # Проверяем, загрузил ли super() данные\n",
        "            try:\n",
        "                # Используем weights_only=False\n",
        "                logging.info(f\"GNN Dataset: Loading pre-processed data from {self.processed_paths[0]}...\")\n",
        "                self.data, self.slices = torch.load(self.processed_paths[0], map_location='cpu', weights_only=False)\n",
        "\n",
        "                # Загружаем scaler, если он не был передан и файл существует\n",
        "                scaler_path = self.root_path / 'scaler.pkl'\n",
        "                if scaler is None and scaler_path.exists():\n",
        "                    self.scaler = joblib.load(scaler_path)\n",
        "                    # logging.info(f\"GNN Dataset: Loaded scaler from {scaler_path}\")\n",
        "                elif scaler is None and not scaler_path.exists():\n",
        "                     logging.error(f\"GNN Dataset: Processed data loaded, but scaler file {scaler_path} not found and no scaler provided.\")\n",
        "                     # raise RuntimeError(\"Scaler missing for pre-processed data.\") # Можно сделать строже\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                 logging.error(f\"GNN Dataset: Processed file {self.processed_paths[0]} should exist but not found.\")\n",
        "                 raise\n",
        "            except Exception as e:\n",
        "                 logging.error(f\"GNN Dataset: Error loading existing processed data/scaler: {e}\")\n",
        "                 raise\n",
        "\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> List[str]: return []\n",
        "    @property\n",
        "    def processed_file_names(self) -> List[str]: return ['data.pt']\n",
        "    @property\n",
        "    def processed_paths(self) -> List[str]: return [str(self.root_path / self.processed_file_names[0])]\n",
        "\n",
        "    # <<< ИЗМЕНЕНИЕ: Добавляем проверку существования файлов >>>\n",
        "    def processed_paths_exist(self) -> bool:\n",
        "        return all(os.path.exists(p) for p in self.processed_paths)\n",
        "\n",
        "    def download(self): pass\n",
        "\n",
        "    # Методы _get_mol, _get_atom_features, _get_bond_features (Без изменений)\n",
        "    # ...\n",
        "    def _get_mol(self, smiles: str) -> Optional[Chem.Mol]:\n",
        "        # ... (код без изменений) ...\n",
        "        try: return Chem.MolFromSmiles(smiles)\n",
        "        except Exception: return None\n",
        "\n",
        "    def _get_atom_features(self, atom: Chem.Atom) -> List[float]:\n",
        "        # ... (код без изменений) ...\n",
        "        hybridization_map = { Chem.rdchem.HybridizationType.SP: [1,0,0,0,0], Chem.rdchem.HybridizationType.SP2: [0,1,0,0,0], Chem.rdchem.HybridizationType.SP3: [0,0,1,0,0], Chem.rdchem.HybridizationType.SP3D: [0,0,0,1,0], Chem.rdchem.HybridizationType.SP3D2: [0,0,0,0,1], Chem.rdchem.HybridizationType.UNSPECIFIED: [0,0,0,0,0], Chem.rdchem.HybridizationType.OTHER: [0,0,0,0,0] }\n",
        "        hybridization = hybridization_map.get(atom.GetHybridization(), [0,0,0,0,0])\n",
        "        chiral_tag = atom.GetChiralTag(); chiral_map = { Chem.rdchem.ChiralType.CHI_UNSPECIFIED: [1,0,0], Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW: [0,1,0], Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW: [0,0,1] }; chirality = chiral_map.get(chiral_tag, [1,0,0])\n",
        "        features = [ float(atom.GetAtomicNum()), float(atom.GetDegree()), float(atom.GetTotalNumHs()), float(atom.GetImplicitValence()), float(atom.GetIsAromatic()), float(atom.GetFormalCharge()), float(atom.IsInRing()) ]\n",
        "        try: gasteiger_charge = float(atom.GetProp('_GasteigerCharge')); gasteiger_charge = 0.0 if np.isnan(gasteiger_charge) or np.isinf(gasteiger_charge) else gasteiger_charge\n",
        "        except Exception: gasteiger_charge = 0.0\n",
        "        features.append(gasteiger_charge); features.extend(hybridization); features.extend(chirality)\n",
        "        return features\n",
        "\n",
        "    def _get_bond_features(self, bond: Chem.Bond) -> List[float]:\n",
        "        # ... (код без изменений) ...\n",
        "        bond_type = bond.GetBondTypeAsDouble(); is_conjugated = float(bond.GetIsConjugated()); is_in_ring = float(bond.IsInRing()); stereo = bond.GetStereo()\n",
        "        stereo_map = { Chem.rdchem.BondStereo.STEREONONE: [1,0,0,0,0,0], Chem.rdchem.BondStereo.STEREOANY: [0,1,0,0,0,0], Chem.rdchem.BondStereo.STEREOZ: [0,0,1,0,0,0], Chem.rdchem.BondStereo.STEREOE: [0,0,0,1,0,0], Chem.rdchem.BondStereo.STEREOCIS: [0,0,0,0,1,0], Chem.rdchem.BondStereo.STEREOTRANS: [0,0,0,0,0,1] }\n",
        "        bond_stereo = stereo_map.get(stereo, [1,0,0,0,0,0]); features = [bond_type, is_conjugated, is_in_ring]; features.extend(bond_stereo)\n",
        "        return features\n",
        "\n",
        "    # Метод process (Без изменений в логике вычислений, но теперь он будет вызван корректно)\n",
        "    def process(self):\n",
        "        # ... (весь код метода process остается таким же, как в предыдущей версии) ...\n",
        "        logging.info(f\"GNN Dataset: Processing data for {self.root}...\")\n",
        "        data_list = []; raw_descriptors_list = []; valid_indices = []; descriptor_names = None\n",
        "        pbar_desc = tqdm_func(self.smiles_list, desc=f\"GNN Descriptors ({self.root.name})\", leave=False)\n",
        "        for i, smi in enumerate(pbar_desc):\n",
        "            desc, names = compute_descriptors_v2(smi)\n",
        "            if desc is not None:\n",
        "                raw_descriptors_list.append(desc); valid_indices.append(i)\n",
        "                if descriptor_names is None and names is not None: descriptor_names = names\n",
        "        if not raw_descriptors_list: raise ValueError(\"GNN Dataset: No valid descriptors computed.\")\n",
        "        computed_descriptors_np = np.array(raw_descriptors_list, dtype=np.float32)\n",
        "        nan_mask = np.isnan(computed_descriptors_np); num_nan = np.sum(nan_mask)\n",
        "        if num_nan > 0: logging.info(f\"GNN Dataset: Imputing {num_nan} NaN values in descriptors with 0.0\")\n",
        "        computed_descriptors_np[nan_mask] = 0.0\n",
        "\n",
        "        if self.fit_scaler:\n",
        "            self.scaler.fit(computed_descriptors_np)\n",
        "            scaler_path = self.root_path / 'scaler.pkl'\n",
        "            joblib.dump(self.scaler, scaler_path)\n",
        "            logging.info(f\"GNN Dataset: Scaler fitted and saved to {scaler_path}\")\n",
        "        elif not hasattr(self.scaler, 'mean_'): raise ValueError(\"GNN Dataset: Scaler needs to be fitted or provided when fit_scaler=False.\")\n",
        "\n",
        "        computed_descriptors_scaled = self.scaler.transform(computed_descriptors_np)\n",
        "        num_descriptor_features = computed_descriptors_scaled.shape[1]\n",
        "        num_atom_f, num_edge_f = -1, -1\n",
        "        pbar_graph = tqdm_func(valid_indices, desc=f\"GNN Graphs ({self.root.name})\", leave=False)\n",
        "        skipped_graphs = 0\n",
        "        for idx, orig_idx in enumerate(pbar_graph):\n",
        "            smi = self.smiles_list[orig_idx]; mol = self._get_mol(smi)\n",
        "            if not mol: skipped_graphs += 1; continue\n",
        "            try:\n",
        "                try: rdPartialCharges.ComputeGasteigerCharges(mol)\n",
        "                except Exception: pass\n",
        "                atom_features_list = [self._get_atom_features(atom) for atom in mol.GetAtoms()]\n",
        "                if not atom_features_list: skipped_graphs += 1; continue\n",
        "                expected_atom_len = GNN_FEATURE_DIMS[0]\n",
        "                if any(len(f) != expected_atom_len for f in atom_features_list):\n",
        "                    logging.warning(f\"GNN: Atom feature length inconsistency for SMILES {smi}. Skipping.\")\n",
        "                    skipped_graphs += 1; continue\n",
        "                atom_features = torch.tensor(atom_features_list, dtype=torch.float32)\n",
        "                if num_atom_f == -1: num_atom_f = atom_features.shape[1]\n",
        "\n",
        "                edge_indices, edge_attrs = [], []\n",
        "                if mol.GetNumBonds() > 0:\n",
        "                    expected_bond_len = GNN_FEATURE_DIMS[1]\n",
        "                    for bond in mol.GetBonds():\n",
        "                        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "                        bond_features = self._get_bond_features(bond)\n",
        "                        if len(bond_features) != expected_bond_len:\n",
        "                             logging.warning(f\"GNN: Bond feature length inconsistency for SMILES {smi}. Using zeros.\")\n",
        "                             bond_features = [0.0] * expected_bond_len\n",
        "                        edge_indices.extend([(i, j), (j, i)])\n",
        "                        edge_attrs.extend([bond_features, bond_features])\n",
        "                    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "                    edge_attr = torch.tensor(edge_attrs, dtype=torch.float32)\n",
        "                    if num_edge_f == -1: num_edge_f = edge_attr.shape[1]\n",
        "                else:\n",
        "                    edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "                    current_num_edge_f = num_edge_f if num_edge_f != -1 else GNN_FEATURE_DIMS[1]\n",
        "                    edge_attr = torch.empty((0, current_num_edge_f), dtype=torch.float32)\n",
        "\n",
        "                desc_tensor = torch.tensor(computed_descriptors_scaled[idx], dtype=torch.float32)\n",
        "                graph_data = Data(x=atom_features, edge_index=edge_index, edge_attr=edge_attr, y=torch.tensor([self.logp_list[orig_idx]], dtype=torch.float32), descriptors=desc_tensor.unsqueeze(0), smiles=smi)\n",
        "                if self.pre_filter is not None and not self.pre_filter(graph_data): skipped_graphs += 1; continue\n",
        "                if self.pre_transform is not None: graph_data = self.pre_transform(graph_data)\n",
        "                data_list.append(graph_data)\n",
        "            except Exception as e: logging.warning(f\"GNN Dataset: Error processing graph for SMILES {smi}: {e}\"); skipped_graphs += 1\n",
        "        if num_edge_f == -1: num_edge_f = 0\n",
        "        if not data_list: raise ValueError(\"GNN Dataset: No graphs were successfully created.\")\n",
        "        if skipped_graphs > 0: logging.warning(f\"GNN Dataset: Skipped {skipped_graphs} graphs during processing.\")\n",
        "\n",
        "        # Важно: Устанавливаем атрибут _processed_feature_dims в конце process()\n",
        "        if num_atom_f != GNN_FEATURE_DIMS[0] or num_edge_f != GNN_FEATURE_DIMS[1] or num_descriptor_features != GNN_FEATURE_DIMS[2]:\n",
        "             logging.warning(f\"GNN Dataset: Final feature dimension mismatch! Expected {GNN_FEATURE_DIMS}, Got ({num_atom_f}, {num_edge_f}, {num_descriptor_features}). Check config/data.\")\n",
        "        self._processed_feature_dims = (num_atom_f, num_edge_f, num_descriptor_features) # <<< УСТАНОВКА АТРИБУТА\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "        logging.info(f\"GNN Dataset: Saving {len(data_list)} processed graphs to {self.processed_paths[0]}...\")\n",
        "        # <<< ИЗМЕНЕНИЕ: Используем weights_only=False при сохранении тоже (на всякий случай) >>>\n",
        "        # Хотя по умолчанию для torch.save это False, но для явности укажем\n",
        "        torch.save((data, slices), self.processed_paths[0], _use_new_zipfile_serialization=False) # _use_new_zipfile... для совместимости?\n",
        "        logging.info(f\"GNN Dataset: Processed feature dimensions: {self._processed_feature_dims}\")\n",
        "\n",
        "\n",
        "    # Метод get_scaler (Без изменений)\n",
        "    def get_scaler(self):\n",
        "        # ... (код без изменений) ...\n",
        "        if not hasattr(self.scaler, 'mean_'):\n",
        "            scaler_path = self.root_path / 'scaler.pkl'\n",
        "            if scaler_path.exists(): self.scaler = joblib.load(scaler_path)\n",
        "            else: raise RuntimeError(\"GNN Dataset: Scaler not fitted/found.\")\n",
        "        return self.scaler\n",
        "\n",
        "    # Метод get_feature_dims (Модифицирован для большей надежности)\n",
        "    def get_feature_dims(self):\n",
        "        # Сначала пытаемся использовать сохраненный атрибут\n",
        "        if hasattr(self, '_processed_feature_dims') and self._processed_feature_dims is not None:\n",
        "             # Дополнительная проверка, что все значения корректны\n",
        "             if all(isinstance(d, int) and d >= 0 for d in self._processed_feature_dims):\n",
        "                  return self._processed_feature_dims\n",
        "\n",
        "        # Если атрибут не установлен или некорректен, пытаемся определить по данным\n",
        "        logging.warning(\"GNN Dataset: _processed_feature_dims not set or invalid. Attempting to determine from data[0].\")\n",
        "        if hasattr(self, 'data') and self.data is not None and len(self) > 0:\n",
        "            try:\n",
        "                d = self.get(0)\n",
        "                num_atom_f = d.x.shape[1] if hasattr(d, 'x') and d.x is not None else -1\n",
        "                num_edge_f = d.edge_attr.shape[1] if hasattr(d, 'edge_attr') and d.edge_attr is not None and d.edge_attr.nelement() > 0 else 0 # 0 если нет связей\n",
        "                num_desc_f = d.descriptors.shape[1] if hasattr(d, 'descriptors') and d.descriptors is not None else -1\n",
        "\n",
        "                if num_atom_f >= 0 and num_edge_f >= 0 and num_desc_f >= 0:\n",
        "                    self._processed_feature_dims = (num_atom_f, num_edge_f, num_desc_f)\n",
        "                    logging.info(f\"GNN Dataset: Determined feature dims from data[0]: {self._processed_feature_dims}\")\n",
        "                    return self._processed_feature_dims\n",
        "                else:\n",
        "                     logging.error(f\"GNN Dataset: Failed to determine valid dims from data[0]. Got ({num_atom_f}, {num_edge_f}, {num_desc_f})\")\n",
        "                     return None # Или вернуть GNN_FEATURE_DIMS как фоллбэк?\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"GNN Dataset: Error determining dims from data[0]: {e}\")\n",
        "                return None\n",
        "        else:\n",
        "            logging.error(\"GNN Dataset: Cannot determine dims - data not loaded or empty.\")\n",
        "            return None\n",
        "\n",
        "# Класс модели LogPPredictorV2\n",
        "class LogPPredictorV2(nn.Module):\n",
        "    # (Без изменений, как в предыдущем блоке кода)\n",
        "    def __init__(self, num_atom_features, num_edge_features, num_descriptor_features, gnn_type='GINE', gat_heads=4, train_eps=True, final_mlp_layers=2, norm_type='LayerNorm', gin_hidden=256, n_layers=4, desc_hidden=512, dropout=0.4, activation_fn='relu', pooling_fn='add'):\n",
        "        super().__init__()\n",
        "        self.num_atom_features = num_atom_features; self.num_edge_features = num_edge_features; self.num_descriptor_features = num_descriptor_features; self.gnn_type = gnn_type\n",
        "        try: self.activation = getattr(F, activation_fn)\n",
        "        except AttributeError: raise ValueError(f\"Activation '{activation_fn}' not found.\")\n",
        "        try: NormLayer = getattr(nn, norm_type)\n",
        "        except AttributeError: raise ValueError(f\"Normalization '{norm_type}' not found.\")\n",
        "        self.pooling_fn_name = pooling_fn\n",
        "        desc_output_dim = desc_hidden // 2\n",
        "        self.desc_mlp = nn.Sequential( nn.Linear(self.num_descriptor_features, desc_hidden), NormLayer(desc_hidden), nn.SiLU(), nn.Dropout(dropout), nn.Linear(desc_hidden, desc_output_dim), NormLayer(desc_output_dim), nn.SiLU(), nn.Dropout(dropout / 2) )\n",
        "        self.convs = nn.ModuleList(); self.batch_norms = nn.ModuleList(); self.skip_lins = nn.ModuleList()\n",
        "        gnn_out_dim = gin_hidden; in_channels = self.num_atom_features\n",
        "        for i in range(n_layers):\n",
        "            current_edge_dim_init = self.num_edge_features if self.num_edge_features > 0 else None\n",
        "            if self.gnn_type == 'GINE':\n",
        "                mlp = nn.Sequential( nn.Linear(in_channels, gin_hidden), nn.ReLU(), nn.Linear(gin_hidden, gin_hidden), nn.BatchNorm1d(gin_hidden) )\n",
        "                self.convs.append(GINEConv(mlp, edge_dim=current_edge_dim_init, train_eps=train_eps)); gnn_out_dim = gin_hidden\n",
        "            elif self.gnn_type == 'GATv2':\n",
        "                self.convs.append(GATv2Conv( in_channels, gin_hidden // gat_heads, heads=gat_heads, dropout=dropout*0.5, edge_dim=current_edge_dim_init, concat=True )); gnn_out_dim = gin_hidden\n",
        "            else: raise ValueError(f\"Unsupported gnn_type: {self.gnn_type}\")\n",
        "            self.batch_norms.append(nn.BatchNorm1d(gnn_out_dim))\n",
        "            if in_channels != gnn_out_dim: self.skip_lins.append(nn.Linear(in_channels, gnn_out_dim))\n",
        "            else: self.skip_lins.append(nn.Identity())\n",
        "            in_channels = gnn_out_dim\n",
        "        self.attention_lin = nn.Linear(gnn_out_dim + desc_output_dim, 1); final_input_dim = gnn_out_dim + desc_output_dim; final_layers = []; current_dim = final_input_dim\n",
        "        for i in range(final_mlp_layers):\n",
        "            out_dim = current_dim // 2 if i < final_mlp_layers - 1 else 1; out_dim = max(1, out_dim)\n",
        "            final_layers.append(nn.Linear(current_dim, out_dim))\n",
        "            if i < final_mlp_layers - 1: final_layers.extend([NormLayer(out_dim), nn.SiLU(), nn.Dropout(dropout / (1.5 + i*0.5))])\n",
        "            current_dim = out_dim\n",
        "        self.final = nn.Sequential(*final_layers); self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if m is self: continue\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                try: # Kaiming for ReLU-like activations\n",
        "                     nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                except ValueError: # Fallback for others (e.g., SiLU, GELU might prefer Xavier)\n",
        "                     nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm1d, nn.LayerNorm)):\n",
        "                 if m.weight is not None: nn.init.ones_(m.weight)\n",
        "                 if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, descriptors, batch = data.x, data.edge_index, data.edge_attr, data.descriptors, data.batch\n",
        "        # Handle cases where edge_attr might be None or empty during inference/processing issues\n",
        "        current_edge_attr = edge_attr if self.num_edge_features > 0 and edge_attr is not None and edge_attr.numel() > 0 else None\n",
        "\n",
        "        x_skip = x\n",
        "        for i in range(len(self.convs)):\n",
        "            x_identity = self.skip_lins[i](x_skip)\n",
        "            # Pass edge_attr only if it's valid\n",
        "            edge_attr_to_pass = current_edge_attr if self.convs[i].__class__.__name__ in ['GINEConv', 'GATv2Conv'] else None # Adapt based on conv needs edge_attr\n",
        "            try:\n",
        "                 x_conv = self.convs[i](x, edge_index, edge_attr=edge_attr_to_pass)\n",
        "            except TypeError as e: # Catch if edge_attr is passed when not needed\n",
        "                 if 'edge_attr' in str(e):\n",
        "                      x_conv = self.convs[i](x, edge_index)\n",
        "                 else: raise e\n",
        "            x = x_conv + x_identity\n",
        "            x = self.batch_norms[i](x)\n",
        "            x = self.activation(x)\n",
        "            x_skip = x\n",
        "\n",
        "        if self.pooling_fn_name == 'add': x_graph = global_add_pool(x, batch)\n",
        "        elif self.pooling_fn_name == 'mean': x_graph = global_mean_pool(x, batch)\n",
        "        elif self.pooling_fn_name == 'max': x_graph = global_max_pool(x, batch)\n",
        "        else: raise ValueError(f\"Unsupported pooling: {self.pooling_fn_name}\")\n",
        "\n",
        "        num_graphs = data.num_graphs\n",
        "        if descriptors.shape[0] != num_graphs:\n",
        "            try: descriptors = descriptors.view(num_graphs, self.num_descriptor_features)\n",
        "            except RuntimeError as e: raise ValueError(f\"Desc batch mismatch: {e}\") from e\n",
        "        x_desc = self.desc_mlp(descriptors); combined_features = torch.cat([x_graph, x_desc], dim=1)\n",
        "        attention_scores = torch.sigmoid(self.attention_lin(combined_features))\n",
        "        gated_graph = attention_scores * x_graph; gated_desc = (1 - attention_scores) * x_desc\n",
        "        combined_gated = torch.cat([gated_graph, gated_desc], dim=1); return self.final(combined_gated)\n",
        "\n",
        "\n",
        "# --- 3. Функции Обучения и Предсказания Базовых Моделей ---\n",
        "\n",
        "def train_predict_svm(X_train, y_train, X_predict, params, output_dir, model_suffix=''):\n",
        "    \"\"\"Обучает SVM и делает предсказания.\"\"\"\n",
        "    logging.info(f\"Training SVM {model_suffix}...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        preprocessor = Pipeline([\n",
        "            ('variance_filter', VarianceThreshold(threshold=params['variance_threshold'])),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "        svr_model = SVR(kernel=params['kernel'], C=params['C'], gamma=params.get('gamma_rbf', 'scale'), cache_size=1000) # Добавим cache_size\n",
        "\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocess', preprocessor),\n",
        "            ('svm', svr_model)\n",
        "        ])\n",
        "\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        logging.info(f\"SVM {model_suffix} training finished in {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # Сохраняем обученный пайплайн\n",
        "        model_filename = f'svm_pipeline{model_suffix}.pkl'\n",
        "        joblib.dump(pipeline, output_dir / model_filename)\n",
        "        logging.info(f\"SVM pipeline saved to {output_dir / model_filename}\")\n",
        "\n",
        "        logging.info(f\"Predicting with SVM {model_suffix}...\")\n",
        "        predictions = pipeline.predict(X_predict)\n",
        "        return predictions, pipeline\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during SVM {model_suffix} train/predict: {e}\")\n",
        "        raise # Передаем ошибку выше\n",
        "\n",
        "def train_predict_lgbm(X_train, y_train, X_predict, params, output_dir, model_suffix=''):\n",
        "    \"\"\"Обучает LightGBM и делает предсказания.\"\"\"\n",
        "    logging.info(f\"Training LightGBM {model_suffix}...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        preprocessing_steps = []\n",
        "        # scaler не используется согласно params\n",
        "        if params.get('use_variance_threshold', False):\n",
        "            preprocessing_steps.append(('variance_filter', VarianceThreshold(threshold=params['variance_threshold'])))\n",
        "\n",
        "        # Убираем параметры препроцессинга из параметров LGBM\n",
        "        lgbm_model_params = {k: v for k, v in params.items() if k not in ['scaler', 'use_variance_threshold', 'variance_threshold']}\n",
        "        lgbm_model = lgb.LGBMRegressor(**lgbm_model_params)\n",
        "\n",
        "        pipeline = Pipeline(preprocessing_steps + [('lgbm', lgbm_model)])\n",
        "        # Передаем DataFrame в fit, чтобы LGBM видел имена фичей, если нужно\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        logging.info(f\"LightGBM {model_suffix} training finished in {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        model_filename = f'lgbm_pipeline{model_suffix}.pkl'\n",
        "        joblib.dump(pipeline, output_dir / model_filename)\n",
        "        logging.info(f\"LGBM pipeline saved to {output_dir / model_filename}\")\n",
        "\n",
        "        logging.info(f\"Predicting with LightGBM {model_suffix}...\")\n",
        "        predictions = pipeline.predict(X_predict)\n",
        "        return predictions, pipeline\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during LGBM {model_suffix} train/predict: {e}\")\n",
        "        raise\n",
        "\n",
        "def train_predict_gnn(train_smiles, train_logp, predict_smiles, params, feature_dims, device, output_dir, mode='oof'):\n",
        "    \"\"\"Обучает GNN и делает предсказания. mode='oof' или 'full'.\"\"\"\n",
        "    logging.info(f\"--- Starting GNN Task (mode: {mode}) ---\")\n",
        "    start_time_task = time.time()\n",
        "\n",
        "    train_root = output_dir / f'gnn_data_{mode}_train'\n",
        "    predict_root = output_dir / f'gnn_data_{mode}_predict'\n",
        "    model_save_path = output_dir / f'gnn_model_{mode}.pth'\n",
        "    scaler_save_path = output_dir / f'gnn_scaler_{mode}.pkl'\n",
        "\n",
        "    actual_dims = None # Инициализируем переменную для размерностей\n",
        "\n",
        "    try:\n",
        "        # 1. Обработка трейн данных и обучение scaler'а\n",
        "        logging.info(f\"GNN ({mode}): Initializing train dataset ({len(train_smiles)} smiles)...\")\n",
        "        # Удаляем старую папку перед созданием, чтобы гарантировать запуск process()\n",
        "        if train_root.exists():\n",
        "             logging.warning(f\"GNN ({mode}): Removing existing train data directory: {train_root}\")\n",
        "             shutil.rmtree(train_root)\n",
        "\n",
        "        train_dataset = LogPInMemoryDataset(root=train_root, smiles_list=train_smiles, logp_list=train_logp, fit_scaler=True)\n",
        "        scaler = train_dataset.get_scaler() # Получаем scaler\n",
        "        logging.info(f\"GNN ({mode}): Train dataset initialized. Root: {train_root}\")\n",
        "\n",
        "        # <<< ИЗМЕНЕНИЕ: Получаем размерности ПОСЛЕ инициализации >>>\n",
        "        logging.info(f\"GNN ({mode}): Determining feature dimensions...\")\n",
        "        actual_dims = train_dataset.get_feature_dims()\n",
        "        if actual_dims is None or any(d < 0 for d in actual_dims): # Проверяем, что размеры корректны\n",
        "             raise ValueError(f\"Failed to determine valid feature dimensions from train_dataset. Got: {actual_dims}\")\n",
        "        logging.info(f\"GNN ({mode}): Actual feature dimensions: {actual_dims}\")\n",
        "        # --- Конец изменения ---\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=GNN_BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "        # 2. Создание и обучение модели GNN\n",
        "        logging.info(f\"GNN ({mode}): Creating and training model...\")\n",
        "        num_atom_f, num_edge_f, num_desc_f = actual_dims # Используем полученные размеры\n",
        "\n",
        "        model = LogPPredictorV2( # Передаем все параметры явно\n",
        "            num_atom_features=num_atom_f, num_edge_features=num_edge_f, num_descriptor_features=num_desc_f,\n",
        "            gnn_type=params.get('gnn_type', 'GINE'), gin_hidden=params['gin_hidden'], n_layers=params['n_layers'],\n",
        "            gat_heads=params.get('gat_heads', 4), train_eps=params.get('train_eps', True),\n",
        "            desc_hidden=params['desc_hidden'], final_mlp_layers=params.get('final_mlp_layers', 2),\n",
        "            norm_type=params.get('norm_type', 'LayerNorm'), dropout=params['dropout'],\n",
        "            activation_fn=params.get('activation', 'relu'), pooling_fn=params.get('pooling', 'add')\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer_class = getattr(torch.optim, params['optimizer'])\n",
        "        optimizer = optimizer_class(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=params['scheduler_T0'], T_mult=params.get('scheduler_T_mult', 2), eta_min=1e-6)\n",
        "        criterion = nn.HuberLoss(delta=params['loss_delta'])\n",
        "\n",
        "        start_time_train = time.time()\n",
        "        model.train()\n",
        "        pbar_epoch = tqdm_func(range(GNN_EPOCHS), desc=f\"GNN Train ({mode})\", leave=False)\n",
        "        for epoch in pbar_epoch:\n",
        "            # ... (внутренний цикл обучения без изменений) ...\n",
        "            epoch_loss = 0; batch_count = 0\n",
        "            for batch in train_loader:\n",
        "                batch = batch.to(device); optimizer.zero_grad()\n",
        "                try:\n",
        "                    out = model(batch); loss = criterion(out, batch.y.view_as(out)); loss.backward()\n",
        "                    clip_norm = params.get('clip_grad_norm')\n",
        "                    if clip_norm is not None: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
        "                    optimizer.step(); epoch_loss += loss.item(); batch_count += 1\n",
        "                except Exception as e: logging.warning(f\"GNN Train Batch Error (Epoch {epoch+1}): {e}\"); continue\n",
        "            scheduler.step()\n",
        "            if batch_count > 0:\n",
        "                 avg_loss = epoch_loss / batch_count\n",
        "                 pbar_epoch.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
        "            else:\n",
        "                 logging.warning(f\"GNN Epoch {epoch+1}: No batches processed.\")\n",
        "\n",
        "\n",
        "        logging.info(f\"GNN ({mode}) training finished in {time.time() - start_time_train:.2f}s\")\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        joblib.dump(scaler, scaler_save_path)\n",
        "        logging.info(f\"GNN model saved to {model_save_path}, scaler to {scaler_save_path}\")\n",
        "\n",
        "        # 3. Обработка данных для предсказания\n",
        "        logging.info(f\"GNN ({mode}): Processing prediction data ({len(predict_smiles)} smiles)...\")\n",
        "         # Удаляем старую папку перед созданием\n",
        "        if predict_root.exists():\n",
        "             logging.warning(f\"GNN ({mode}): Removing existing prediction data directory: {predict_root}\")\n",
        "             shutil.rmtree(predict_root)\n",
        "        dummy_logp = [0.0] * len(predict_smiles)\n",
        "        predict_dataset = LogPInMemoryDataset(root=predict_root, smiles_list=predict_smiles, logp_list=dummy_logp, fit_scaler=False, scaler=scaler)\n",
        "        predict_loader = DataLoader(predict_dataset, batch_size=GNN_BATCH_SIZE * 2, shuffle=False, num_workers=0)\n",
        "        logging.info(f\"GNN ({mode}): Prediction data processed.\")\n",
        "\n",
        "        # 4. Получение предсказаний\n",
        "        # ... (код предсказания без изменений) ...\n",
        "        logging.info(f\"GNN ({mode}): Predicting...\")\n",
        "        model.eval()\n",
        "        predictions_list = []\n",
        "        pbar_pred = tqdm_func(predict_loader, desc=f\"GNN Predict ({mode})\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in pbar_pred:\n",
        "                batch = batch.to(device)\n",
        "                try: pred = model(batch); predictions_list.extend(pred.cpu().numpy().flatten())\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"GNN Predict Batch Error: {e}\")\n",
        "                    num_in_batch = batch.y.size(0); predictions_list.extend([np.nan] * num_in_batch); continue\n",
        "        predictions = np.array(predictions_list)\n",
        "        if np.isnan(predictions).any():\n",
        "            num_nan = np.isnan(predictions).sum()\n",
        "            logging.warning(f\"GNN ({mode}): Found {num_nan} NaN predictions. Imputing with mean.\")\n",
        "            mean_pred = np.nanmean(predictions); fill_value = mean_pred if not np.isnan(mean_pred) else 0.0\n",
        "            predictions[np.isnan(predictions)] = fill_value\n",
        "\n",
        "\n",
        "        logging.info(f\"--- GNN Task (mode: {mode}) finished in {time.time() - start_time_task:.2f}s ---\")\n",
        "        return predictions, model, scaler\n",
        "    except Exception as e:\n",
        "         logging.error(f\"Error during GNN task (mode: {mode}): {e}\")\n",
        "         import traceback\n",
        "         traceback.print_exc()\n",
        "         raise\n",
        "\n",
        "# --- 4. Основной Пайплайн Стекинга ---\n",
        "def run_stacking():\n",
        "    logging.info(\"--- STARTING STACKING PIPELINE v2 ---\")\n",
        "\n",
        "    # --- 4.1 Загрузка исходных данных (С ГЕНЕРАЦИЕЙ ID) ---\n",
        "    logging.info(\"Loading source data...\")\n",
        "    try:\n",
        "        df_train_source = pd.read_csv(SOURCE_TRAIN_DATA_PATH)\n",
        "        df_test_source = pd.read_csv(SOURCE_TEST_DATA_PATH)\n",
        "        df_sample_sub = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
        "\n",
        "        # Проверка и генерация ID для трейна\n",
        "        if ID_COLUMN not in df_train_source.columns:\n",
        "            logging.warning(f\"Column '{ID_COLUMN}' not found in train source. Generating ID from index.\")\n",
        "            df_train_source[ID_COLUMN] = df_train_source.index\n",
        "        # Проверка остальных колонок трейна\n",
        "        if SMILES_COLUMN not in df_train_source.columns or TARGET_COLUMN not in df_train_source.columns:\n",
        "            raise ValueError(f\"Missing '{SMILES_COLUMN}' or '{TARGET_COLUMN}' in train source.\")\n",
        "\n",
        "        # Проверка и генерация ID для теста\n",
        "        if ID_COLUMN not in df_test_source.columns:\n",
        "            logging.warning(f\"Column '{ID_COLUMN}' not found in test source. Generating ID from index.\")\n",
        "            df_test_source[ID_COLUMN] = df_test_source.index\n",
        "        # Проверка SMILES в тесте\n",
        "        if SMILES_COLUMN not in df_test_source.columns:\n",
        "             raise ValueError(f\"Missing '{SMILES_COLUMN}' in test source.\")\n",
        "        # Проверка ID в sample submission\n",
        "        if ID_COLUMN not in df_sample_sub.columns:\n",
        "             raise ValueError(f\"Missing '{ID_COLUMN}' in sample submission.\")\n",
        "\n",
        "        logging.info(f\"Train source: {df_train_source.shape}, Test source: {df_test_source.shape}\")\n",
        "    except Exception as e: logging.error(f\"Failed to load or process source data: {e}\"); return\n",
        "\n",
        "    # --- 4.2 Загрузка данных с признаками для SVM/LGBM ---\n",
        "    logging.info(\"Loading feature data for SVM/LGBM...\")\n",
        "    try:\n",
        "        df_train_features = pd.read_csv(SVM_LGBM_TRAIN_FEATURES_PATH)\n",
        "        df_test_features = pd.read_csv(SVM_LGBM_TEST_FEATURES_PATH)\n",
        "\n",
        "        # Определяем имена признаков (все колонки минус таргет)\n",
        "        if TARGET_COLUMN not in df_train_features.columns:\n",
        "             raise ValueError(f\"Target column '{TARGET_COLUMN}' not found in train features file.\")\n",
        "        svm_lgbm_feature_cols = [col for col in df_train_features.columns if col != TARGET_COLUMN]\n",
        "        if len(svm_lgbm_feature_cols) != 201:\n",
        "             raise ValueError(f\"Expected 201 features for SVM/LGBM, found {len(svm_lgbm_feature_cols)}\")\n",
        "\n",
        "        # Проверяем, есть ли колонка ID в тестовых признаках (иногда бывает)\n",
        "        if ID_COLUMN in df_test_features.columns and len(df_test_features.columns) == 202:\n",
        "             logging.info(f\"Removing '{ID_COLUMN}' column from test features.\")\n",
        "             X_svm_lgbm_test_raw = df_test_features.drop(columns=[ID_COLUMN])\n",
        "             # Убедимся, что порядок колонок совпадает с трейном\n",
        "             X_svm_lgbm_test_raw = X_svm_lgbm_test_raw[svm_lgbm_feature_cols]\n",
        "        elif len(df_test_features.columns) == 201:\n",
        "             # Предполагаем, что это только признаки и порядок верный\n",
        "             X_svm_lgbm_test_raw = df_test_features[svm_lgbm_feature_cols]\n",
        "        else:\n",
        "             raise ValueError(f\"Unexpected number of columns in test features file: {len(df_test_features.columns)}\")\n",
        "\n",
        "\n",
        "        # --- СОПОСТАВЛЕНИЕ ПРИЗНАКОВ С ID/SMILES (Упрощенный вариант, если порядок совпадает) ---\n",
        "        if len(df_train_source) != len(df_train_features) or len(df_test_source) != len(df_test_features):\n",
        "             logging.error(\"Row count mismatch between source files and feature files. Cannot proceed with simple alignment.\")\n",
        "             # Здесь нужна была бы более сложная логика merge по SMILES или др. ключам, если порядок не совпадает.\n",
        "             # Для простоты пока предполагаем, что порядок совпадает.\n",
        "             return\n",
        "\n",
        "        logging.info(\"Aligning features based on assumed matching row order...\")\n",
        "        # Добавляем ID из source к feature dataframes\n",
        "        df_train_features_with_id = df_train_features.copy()\n",
        "        df_train_features_with_id[ID_COLUMN] = df_train_source[ID_COLUMN].values\n",
        "\n",
        "        df_test_features_with_id = X_svm_lgbm_test_raw.copy() # X_svm_lgbm_test_raw уже содержит только фичи\n",
        "        df_test_features_with_id[ID_COLUMN] = df_test_source[ID_COLUMN].values\n",
        "\n",
        "        logging.info(f\"SVM/LGBM Train Features + ID: {df_train_features_with_id.shape}\")\n",
        "        logging.info(f\"SVM/LGBM Test Features + ID: {df_test_features_with_id.shape}\")\n",
        "\n",
        "    except Exception as e: logging.error(f\"Failed to load or process SVM/LGBM feature data: {e}\"); return\n",
        "\n",
        "    # --- 4.3 Единое разделение данных ---\n",
        "    logging.info(\"Performing unified train/validation split...\")\n",
        "    # Разделяем ИСХОДНЫЙ DataFrame С ID\n",
        "    train_ids, val_ids = train_test_split(\n",
        "        df_train_source[ID_COLUMN].values, # Разделяем по ID\n",
        "        test_size=VALIDATION_SET_SIZE,\n",
        "        random_state=SPLIT_RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "    # Создаем DataFrame'ы для трейна и валидации на основе ID\n",
        "    train_base_df = df_train_source[df_train_source[ID_COLUMN].isin(train_ids)].copy()\n",
        "    val_df = df_train_source[df_train_source[ID_COLUMN].isin(val_ids)].copy()\n",
        "    y_val_true = val_df[TARGET_COLUMN].values\n",
        "    np.save(OUTPUT_DIR / 'y_val_true.npy', y_val_true) # Сохраняем сразу\n",
        "\n",
        "    # Отбираем признаки SVM/LGBM для OOF по ID\n",
        "    X_svm_lgbm_train_base = df_train_features_with_id[df_train_features_with_id[ID_COLUMN].isin(train_ids)][svm_lgbm_feature_cols]\n",
        "    y_svm_lgbm_train_base = df_train_features_with_id[df_train_features_with_id[ID_COLUMN].isin(train_ids)][TARGET_COLUMN]\n",
        "    X_svm_lgbm_val = df_train_features_with_id[df_train_features_with_id[ID_COLUMN].isin(val_ids)][svm_lgbm_feature_cols]\n",
        "\n",
        "    logging.info(f\"Split complete: Train base size: {len(train_base_df)}, Validation size: {len(val_df)}\")\n",
        "    logging.info(f\"SVM/LGBM features shapes: Train base {X_svm_lgbm_train_base.shape}, Val {X_svm_lgbm_val.shape}\")\n",
        "    if len(X_svm_lgbm_val) != len(val_df):\n",
        "         logging.error(\"Mismatch between validation set size and selected SVM/LGBM features for validation!\")\n",
        "         return\n",
        "\n",
        "    # --- 4.4 Генерация OOF Предсказаний ---\n",
        "    logging.info(\"--- Generating OOF Predictions (Level 0) ---\")\n",
        "    oof_preds = {}\n",
        "    # Словарь для хранения полных моделей/пайплайнов, обученных на ВСЕХ трейн данных\n",
        "    full_trained_models = {}\n",
        "\n",
        "    # SVM OOF\n",
        "    try:\n",
        "        svm_oof_preds, _ = train_predict_svm( # Пайплайн для OOF не сохраняем\n",
        "            X_svm_lgbm_train_base.values, # Передаем NumPy\n",
        "            y_svm_lgbm_train_base.values,\n",
        "            X_svm_lgbm_val.values,\n",
        "            BEST_SVM_PARAMS,\n",
        "            OUTPUT_DIR,\n",
        "            model_suffix='_oof'\n",
        "        )\n",
        "        oof_preds['svm'] = svm_oof_preds\n",
        "        np.save(OUTPUT_DIR / 'svm_oof.npy', svm_oof_preds)\n",
        "        logging.info(f\"SVM OOF generation complete. Shape: {svm_oof_preds.shape}\")\n",
        "    except Exception as e: logging.error(f\"SVM OOF generation failed: {e}\"); return\n",
        "\n",
        "    # LGBM OOF\n",
        "    try:\n",
        "        lgbm_oof_preds, _ = train_predict_lgbm(\n",
        "            X_svm_lgbm_train_base, # Передаем DataFrame\n",
        "            y_svm_lgbm_train_base,\n",
        "            X_svm_lgbm_val,\n",
        "            BEST_LGBM_PARAMS,\n",
        "            OUTPUT_DIR,\n",
        "            model_suffix='_oof'\n",
        "        )\n",
        "        oof_preds['lgbm'] = lgbm_oof_preds\n",
        "        np.save(OUTPUT_DIR / 'lgbm_oof.npy', lgbm_oof_preds)\n",
        "        logging.info(f\"LGBM OOF generation complete. Shape: {lgbm_oof_preds.shape}\")\n",
        "    except Exception as e: logging.error(f\"LGBM OOF generation failed: {e}\"); return\n",
        "\n",
        "    # GNN OOF\n",
        "    try:\n",
        "        gnn_oof_preds, _, _ = train_predict_gnn( # Модель и scaler для OOF не сохраняем в основной словарь\n",
        "            train_base_df[SMILES_COLUMN].tolist(),\n",
        "            train_base_df[TARGET_COLUMN].tolist(),\n",
        "            val_df[SMILES_COLUMN].tolist(),\n",
        "            BEST_HYPERPARAMS_GNN,\n",
        "            GNN_FEATURE_DIMS,\n",
        "            GNN_DEVICE,\n",
        "            OUTPUT_DIR,\n",
        "            mode='oof'\n",
        "        )\n",
        "        oof_preds['gnn'] = gnn_oof_preds\n",
        "        np.save(OUTPUT_DIR / 'gnn_oof.npy', gnn_oof_preds)\n",
        "        logging.info(f\"GNN OOF generation complete. Shape: {gnn_oof_preds.shape}\")\n",
        "    except Exception as e: logging.error(f\"GNN OOF generation failed: {e}\"); return\n",
        "\n",
        "    # Проверка размерностей OOF\n",
        "    oof_lengths = {name: len(pred) for name, pred in oof_preds.items()}\n",
        "    if not (oof_lengths['svm'] == oof_lengths['lgbm'] == oof_lengths['gnn'] == len(y_val_true)):\n",
        "        logging.error(f\"OOF predictions size mismatch! Lengths: {oof_lengths}, Expected: {len(y_val_true)}\")\n",
        "        return\n",
        "\n",
        "    # --- 4.5 Обучение Мета-Модели (Ridge + Optuna) ---\n",
        "    logging.info(\"--- Training Meta-Model (Level 1) ---\")\n",
        "    meta_model = None\n",
        "    try:\n",
        "        X_meta_train = pd.DataFrame({\n",
        "            'svm_pred': oof_preds['svm'], 'lgbm_pred': oof_preds['lgbm'], 'gnn_pred': oof_preds['gnn'],\n",
        "            'diff_svm_gnn': np.abs(oof_preds['svm'] - oof_preds['gnn']),\n",
        "            'diff_lgbm_gnn': np.abs(oof_preds['lgbm'] - oof_preds['gnn']),\n",
        "            'diff_svm_lgbm': np.abs(oof_preds['svm'] - oof_preds['lgbm']),\n",
        "            'svm_pred_sq': oof_preds['svm']**2, 'lgbm_pred_sq': oof_preds['lgbm']**2, 'gnn_pred_sq': oof_preds['gnn']**2,\n",
        "        })\n",
        "        y_meta_train = y_val_true\n",
        "        logging.info(f\"Meta-features (train) created. Shape: {X_meta_train.shape}\")\n",
        "\n",
        "        def objective_ridge(trial):\n",
        "            alpha = trial.suggest_float('alpha', 1e-4, 1e4, log=True)\n",
        "            model = Ridge(alpha=alpha, random_state=SPLIT_RANDOM_STATE)\n",
        "            kfold = KFold(n_splits=META_MODEL_CV_FOLDS, shuffle=True, random_state=trial.number)\n",
        "            scores = cross_val_score(model, X_meta_train, y_meta_train, cv=kfold, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
        "            return np.mean(scores)\n",
        "\n",
        "        study_ridge = optuna.create_study(direction='maximize')\n",
        "        logging.info(f\"Optimizing Ridge alpha ({META_MODEL_OPTUNA_TRIALS} trials)...\")\n",
        "        study_ridge.optimize(objective_ridge, n_trials=META_MODEL_OPTUNA_TRIALS, timeout=META_MODEL_OPTUNA_TIMEOUT)\n",
        "\n",
        "        if study_ridge.best_trial is None: raise RuntimeError(\"Optuna failed to find parameters for Ridge.\")\n",
        "        best_alpha = study_ridge.best_params['alpha']\n",
        "        best_cv_rmse = -study_ridge.best_value\n",
        "        logging.info(f\"Ridge optimization complete. Best CV RMSE: {best_cv_rmse:.5f}, Best alpha: {best_alpha:.6f}\")\n",
        "\n",
        "        meta_model = Ridge(alpha=best_alpha, random_state=SPLIT_RANDOM_STATE)\n",
        "        meta_model.fit(X_meta_train, y_meta_train)\n",
        "        logging.info(\"Final Meta-Model (Ridge) trained.\")\n",
        "        joblib.dump(meta_model, OUTPUT_DIR / 'meta_model_ridge.pkl')\n",
        "    except Exception as e: logging.error(f\"Meta-Model training failed: {e}\"); return\n",
        "\n",
        "    # --- 4.6 Генерация Тестовых Предсказаний Базовых Моделей ---\n",
        "    logging.info(\"--- Generating Test Predictions (Level 0) ---\")\n",
        "    test_preds = {}\n",
        "\n",
        "    # Данные для обучения полных моделей\n",
        "    X_train_full_svm_lgbm = df_train_features_with_id[svm_lgbm_feature_cols]\n",
        "    y_train_full_svm_lgbm = df_train_features_with_id[TARGET_COLUMN]\n",
        "    train_smiles_full = df_train_source[SMILES_COLUMN].tolist()\n",
        "    train_logp_full = df_train_source[TARGET_COLUMN].tolist()\n",
        "    # Тестовые данные\n",
        "    X_test_svm_lgbm = df_test_features_with_id[svm_lgbm_feature_cols]\n",
        "    test_smiles = df_test_source[SMILES_COLUMN].tolist()\n",
        "\n",
        "    # SVM Test Preds (Обучение на полном трейне)\n",
        "    try:\n",
        "        logging.info(\"Training FULL SVM and predicting on test...\")\n",
        "        svm_test_preds, full_svm_pipeline = train_predict_svm(\n",
        "            X_train_full_svm_lgbm.values, y_train_full_svm_lgbm.values, X_test_svm_lgbm.values,\n",
        "            BEST_SVM_PARAMS, OUTPUT_DIR, model_suffix='_full'\n",
        "        )\n",
        "        test_preds['svm'] = svm_test_preds\n",
        "        np.save(OUTPUT_DIR / 'svm_test_preds.npy', svm_test_preds)\n",
        "        logging.info(f\"SVM Test predictions generated. Shape: {svm_test_preds.shape}\")\n",
        "    except Exception as e: logging.error(f\"SVM Full training/Test prediction failed: {e}\"); return\n",
        "\n",
        "    # LGBM Test Preds (Обучение на полном трейне)\n",
        "    try:\n",
        "        logging.info(\"Training FULL LGBM and predicting on test...\")\n",
        "        lgbm_test_preds, full_lgbm_pipeline = train_predict_lgbm(\n",
        "            X_train_full_svm_lgbm, y_train_full_svm_lgbm, X_test_svm_lgbm,\n",
        "            BEST_LGBM_PARAMS, OUTPUT_DIR, model_suffix='_full'\n",
        "        )\n",
        "        test_preds['lgbm'] = lgbm_test_preds\n",
        "        np.save(OUTPUT_DIR / 'lgbm_test_preds.npy', lgbm_test_preds)\n",
        "        logging.info(f\"LGBM Test predictions generated. Shape: {lgbm_test_preds.shape}\")\n",
        "    except Exception as e: logging.error(f\"LGBM Full training/Test prediction failed: {e}\"); return\n",
        "\n",
        "    # GNN Test Preds (Обучение на полном трейне)\n",
        "    try:\n",
        "        logging.info(\"Training FULL GNN and predicting on test...\")\n",
        "        gnn_test_preds, full_gnn_model, full_gnn_scaler = train_predict_gnn(\n",
        "            train_smiles_full, train_logp_full, test_smiles,\n",
        "            BEST_HYPERPARAMS_GNN, GNN_FEATURE_DIMS, GNN_DEVICE,\n",
        "            OUTPUT_DIR, mode='full' # Используем другой режим/папку\n",
        "        )\n",
        "        test_preds['gnn'] = gnn_test_preds\n",
        "        np.save(OUTPUT_DIR / 'gnn_test_preds.npy', gnn_test_preds)\n",
        "        logging.info(f\"GNN Test predictions generated. Shape: {gnn_test_preds.shape}\")\n",
        "        # Сохраняем полную модель GNN и scaler\n",
        "        # torch.save(full_gnn_model.state_dict(), OUTPUT_DIR / 'gnn_model_full.pth') # Уже сохранено в функции\n",
        "        # joblib.dump(full_gnn_scaler, OUTPUT_DIR / 'gnn_scaler_full.pkl') # Уже сохранено в функции\n",
        "    except Exception as e: logging.error(f\"GNN Full training/Test prediction failed: {e}\"); return\n",
        "\n",
        "    # Проверка размеров тестовых предсказаний\n",
        "    if not (len(test_preds['svm']) == len(test_preds['lgbm']) == len(test_preds['gnn']) == len(df_test_source)):\n",
        "        logging.error(f\"Test predictions size mismatch! SVM:{len(test_preds['svm'])}, LGBM:{len(test_preds['lgbm'])}, GNN:{len(test_preds['gnn'])}, Expected:{len(df_test_source)}\")\n",
        "        return\n",
        "\n",
        "    # --- 4.7 Финальное Предсказание Мета-Моделью ---\n",
        "    logging.info(\"--- Generating Final Predictions (Level 1) ---\")\n",
        "    try:\n",
        "        X_meta_test = pd.DataFrame({\n",
        "            'svm_pred': test_preds['svm'], 'lgbm_pred': test_preds['lgbm'], 'gnn_pred': test_preds['gnn'],\n",
        "            'diff_svm_gnn': np.abs(test_preds['svm'] - test_preds['gnn']),\n",
        "            'diff_lgbm_gnn': np.abs(test_preds['lgbm'] - test_preds['gnn']),\n",
        "            'diff_svm_lgbm': np.abs(test_preds['svm'] - test_preds['lgbm']),\n",
        "            'svm_pred_sq': test_preds['svm']**2, 'lgbm_pred_sq': test_preds['lgbm']**2, 'gnn_pred_sq': test_preds['gnn']**2,\n",
        "        })\n",
        "        logging.info(f\"Meta-features (test) created. Shape: {X_meta_test.shape}\")\n",
        "\n",
        "        # Загрузка мета-модели (если запускается отдельно)\n",
        "        if meta_model is None:\n",
        "            meta_model = joblib.load(OUTPUT_DIR / 'meta_model_ridge.pkl')\n",
        "            logging.info(\"Reloaded meta-model.\")\n",
        "\n",
        "        final_predictions = meta_model.predict(X_meta_test)\n",
        "        logging.info(f\"Final predictions generated. Shape: {final_predictions.shape}\")\n",
        "    except Exception as e: logging.error(f\"Final prediction failed: {e}\"); return\n",
        "\n",
        "    # --- 4.8 Формирование Сабмишена ---\n",
        "    logging.info(\"--- Creating Submission File ---\")\n",
        "    try:\n",
        "        # Используем ID из df_test_source, где мы его сгенерировали, если было нужно\n",
        "        final_sub_ids = df_test_source[ID_COLUMN].values\n",
        "        if len(final_sub_ids) != len(final_predictions):\n",
        "             raise ValueError(f\"Length mismatch between final predictions ({len(final_predictions)}) and test IDs ({len(final_sub_ids)})\")\n",
        "\n",
        "        submission_df = pd.DataFrame({ ID_COLUMN: final_sub_ids, TARGET_COLUMN: final_predictions })\n",
        "\n",
        "        # --- Важно: Сопоставление с ID из Sample Submission ---\n",
        "        # Загружаем sample submission ЕЩЕ РАЗ, чтобы гарантировать правильные ID и порядок\n",
        "        df_sample_sub_final = pd.read_csv(SAMPLE_SUBMISSION_PATH)[[ID_COLUMN]] # Берем только ID\n",
        "        # Объединяем наши предсказания с sample submission ID\n",
        "        final_submission_df = df_sample_sub_final.merge(submission_df, on=ID_COLUMN, how='left')\n",
        "\n",
        "        # Проверка на пропуски после merge (если ID не совпали)\n",
        "        if final_submission_df[TARGET_COLUMN].isnull().any():\n",
        "             logging.warning(\"NaN values found in final submission after merging with sample! Check ID matching.\")\n",
        "             # Можно попробовать заполнить пропуски средним или медианой предсказаний, но это плохой знак\n",
        "             # final_submission_df[TARGET_COLUMN].fillna(np.median(final_predictions), inplace=True)\n",
        "\n",
        "        # Проверка итогового размера\n",
        "        if len(final_submission_df) != len(df_sample_sub):\n",
        "            logging.warning(f\"Final submission size ({len(final_submission_df)}) differs from sample submission size ({len(df_sample_sub)}).\")\n",
        "\n",
        "        submission_path = OUTPUT_DIR / 'submission_stacking_svm_lgbm_gnn_v2.csv'\n",
        "        final_submission_df[[ID_COLUMN, TARGET_COLUMN]].to_csv(submission_path, index=False) # Сохраняем только 2 колонки\n",
        "        logging.info(f\"Submission file saved to: {submission_path}\")\n",
        "        logging.info(\"Submission head:\\n\" + final_submission_df.head().to_string())\n",
        "\n",
        "    except Exception as e: logging.error(f\"Failed to create submission file: {e}\"); return\n",
        "\n",
        "    logging.info(\"--- STACKING PIPELINE FINISHED SUCCESSFULLY ---\")\n",
        "\n",
        "# --- Запуск Пайплайна ---\n",
        "if __name__ == \"__main__\":\n",
        "    if 'ipykernel' not in sys.modules:\n",
        "        tqdm_func = tqdm_base\n",
        "        print(\"Running as script, using standard tqdm.\")\n",
        "    else:\n",
        "         # Убедимся, что tqdm_func определен и в ноутбуке\n",
        "         try: from tqdm.notebook import tqdm as tqdm_notebook; tqdm_func = tqdm_notebook\n",
        "         except ImportError: from tqdm import tqdm as tqdm_base; tqdm_func = tqdm_base\n",
        "\n",
        "    run_stacking()"
      ],
      "metadata": {
        "id": "gm3wA4p07XlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C9W8MTYf9_jT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}